

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>DNN (Deep Neural Networks) &#8212; AI primer</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link rel="stylesheet" href="_static/jupyterbook.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <link href="_static/css/index.css" rel="stylesheet">
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide_output div.cell_output, .tag_hide_cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="SVD (Singular Value Decomposition)" href="SVD.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
    
<div class='top'>
<button id="navbar-toggler" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation">
    <i class="fas fa-bars"></i>
    <i class="fas fa-arrow-left"></i>
    <i class="fas fa-arrow-up"></i>
</button>
</div>

    <div class="container-fluid">
      <div class="row">
          
<div class="col-12 col-md-3 col-xl-3 bd-sidebar show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">AI primer</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="RF.html">RF (Random Forest)</a>
  </li>
  <li class="">
    <a href="SVD.html">SVD (Singular Value Decomposition)</a>
  </li>
  <li class="active">
    <a href="">DNN (Deep Neural Networks)</a>
  </li>
</ul>
</nav>
<p class="sidebar_footer">Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a></p>
</div>

          
<div class="d-none d-xl-block col-xl-2 bd-toc">


<div class="tocsection onthispage"><i class="fas fa-list"></i> On this page</div>
<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-perceptron" class="nav-link">The Perceptron</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#perceptron-for-2-class-case" class="nav-link">Perceptron for 2 class case</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#beginning-of-first-ai-winter" class="nav-link">Beginning of first AI winter</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#backpropagation" class="nav-link">Backpropagation</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#two-layer-nn-for-2-class-case" class="nav-link">Two layer NN for 2 class case</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#keras" class="nav-link">Keras</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#mnist-application-of-backprop-by-yann-lecun-1989" class="nav-link">MNIST (Application of backprop by Yann LeCun, 1989)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#a-note-on-gradien-descent" class="nav-link">A note on Gradien Descent</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#a-note-on-dropout" class="nav-link">A note on Dropout</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#a-note-on-architectures" class="nav-link">A note on Architectures</a>
        </li>
    
    </ul>
</nav>
</div>


          
<main class="col py-md-3 pl-md-5 pr-0 bd-content overflow-auto" role="main">
    
    <div class="row topbar sticky-top">
    <button id="navbar-toggler" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation">
        <i class="fas fa-bars"></i>
        <i class="fas fa-arrow-left"></i>
    </button>
    <a class="download-buttons" href="_sources/DNN.ipynb.txt"><button type="button" class="btn btn-secondary topbarbtn"><i class="fas fa-download"></i>.ipynb</button></a>
    
</div>
    <div class="row">
        <div class="col-12 ">
        
              <div>
                
  <div class="section" id="dnn-deep-neural-networks">
<h1>DNN (Deep Neural Networks)<a class="headerlink" href="#dnn-deep-neural-networks" title="Permalink to this headline">¶</a></h1>
<img src="https://cdn-images-1.medium.com/max/2000/1*Z_DnCyKt18RM0aCCrFzaIQ.png" style="width: 80%"/><p>ANNs have been around for quite a while: they were first introduced back in <strong>1943</strong> by the neurophysiologist Warren <strong>McCulloch</strong> and the mathematician Walter <strong>Pitts</strong> (see “A Logical Calculus of Ideas Immanent in Nervous Activity”).</p>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571117028252_image.png" style="width: 50%"/><img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571117050006_image.png" style="width: 50%"/><p>Most of the pictures are taken from the great book “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition” by Aurélien Géron.</p>
<div class="section" id="the-perceptron">
<h2>The Perceptron<a class="headerlink" href="#the-perceptron" title="Permalink to this headline">¶</a></h2>
<p>The <em>Perceptron</em> is one of the simplest ANN architectures, invented in <strong>1957</strong> by Frank <strong>Rosenblatt</strong>. It is based on a slightly different artificial neuron called a <em>threshold logic unit</em> (TLU), or sometimes a <em>linear threshold unit</em> (LTU). A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs.</p>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571117134137_image.png" alt="Threshold logic unit: an artificial neuron which computes a weighted sum of its inputs then applies a step function" style="width: 50%"/><img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571117253013_image.png" alt="Architecture of a Perceptron with two input neurons, one bias neuron, and three output neurons" style="width: 50%"/><p>Lets see how is a Perceptron trained.</p>
<p>The Perceptron training algorithm proposed by <strong>Rosenblatt</strong> was largely inspired by <em><strong>Hebb’s rule</strong></em>. In his <strong>1949</strong> book <em>The Organization of Behavior</em> (Wiley), Donald Hebb suggested that when a biological neuron triggers another neuron often, the connection between these two neurons grows stronger. Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire together, wire together”; that is, the connection weight between two neurons tends to increase when they fire simultaneously. This rule later became known as Hebb’s rule (or <em>Hebbian learning</em>).</p>
<img src="https://pbs.twimg.com/media/DARmL4KXYAAj0td.jpg:large" style="width: 40%"/><p>The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers). However, if the training instances are linearly separable, <strong>Rosenblatt</strong> demonstrated that this algorithm would converge to a solution. This is called the <strong>Perceptron convergence theorem</strong>.</p>
<div class="row">
  <div class="column">
    <img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571117522486_image.png" style="width: 30%"/>
  </div>
  <div class="column">
    <img src="https://www.manhattanrarebooks.com/pictures/227.jpg?v=1354507081" style="width: 30%"/>
  </div>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">cufflinks</span> <span class="k">as</span> <span class="nn">cf</span>  <span class="c1"># nicer plots, see https://plot.ly/ipython-notebooks/cufflinks/</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">cf</span><span class="o">.</span><span class="n">go_offline</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="mi">46790</span><span class="n">fdc9119</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="kn">import</span> <span class="nn">cufflinks</span> <span class="k">as</span> <span class="nn">cf</span>  <span class="c1"># nicer plots, see https://plot.ly/ipython-notebooks/cufflinks/</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;cufflinks&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;pc_1&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;pc_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">iplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;pc_1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;pc_2&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="s1">&#39;class&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div id="d8be87b7-bca4-4d18-b622-73393e85188d" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("d8be87b7-bca4-4d18-b622-73393e85188d", [{"type": "scatter", "x": [-2.684125625969542, -2.7141416872943265, -2.888990569059299, -2.745342855641412, -2.7287165365545323, -2.2808596328444932, -2.8205377507406104, -2.6261449731466344, -2.886382731780555, -2.6727557978209555, -2.5069470906518574, -2.6127552309087245, -2.7861092661880194, -3.2238037438656546, -2.6447503899420304, -2.386039033531135, -2.6235278752244278, -2.648296706254383, -2.199820323617581, -2.5879863998787704, -2.3102562152425192, -2.543705228757158, -3.2159394156486125, -2.3027331822262083, -2.355754049123774, -2.5066689069258237, -2.4688200731213406, -2.562319906196018, -2.6395347153845443, -2.6319893872743476, -2.587398476689354, -2.4099324970021763, -2.6488623343499134, -2.598736749100589, -2.6369268781058004, -2.866241652118671, -2.625238049850374, -2.8006841154482234, -2.9805020437819953, -2.5900063139680976, -2.7701024260279037, -2.849368705043106, -2.9974065465949096, -2.4056144850974874, -2.209489237783681, -2.714451426757709, -2.538148258998942, -2.8394621676428518, -2.5430857498303947, -2.703359782335162], "y": [0.31939724658510305, -0.1770012250647806, -0.14494942608555744, -0.3182989792519163, 0.3267545129349198, 0.741330449062915, -0.08946138452856905, 0.16338495969832867, -0.5783117541867042, -0.11377424587411686, 0.6450688986485742, 0.01472993916137436, -0.2351120002017185, -0.5113945870063825, 1.1787646364375757, 1.3380623304006531, 0.8106795141812578, 0.31184914459335483, 0.8728390389622113, 0.513560308749277, 0.39134593565389447, 0.43299606327902823, 0.13346806953852564, 0.09870885481409927, -0.037281859677382734, -0.14601688049526776, 0.13095148943525017, 0.3677188574342001, 0.31203998023528307, -0.19696122492431467, -0.20431849127413357, 0.4109242642295731, 0.8133638202969622, 1.0931457594493572, -0.12132234786586323, 0.06936447158008058, 0.5993700213794241, 0.2686437377979823, -0.4879583444286158, 0.22904383682701268, 0.26352753374425647, -0.940960573641197, -0.3419260574716102, 0.18887142893026015, 0.43666314163918785, -0.2502082041852113, 0.5037711444614377, -0.22794556949382783, 0.5794100215198895, 0.10770608249941162], "mode": "markers", "name": "setosa", "marker": {"color": "rgba(255, 153, 51, 1.0)", "symbol": "dot", "size": 12, "opacity": 0.8, "line": {"width": 1.3}}, "textfont": {"color": "#4D5663"}}, {"type": "scatter", "x": [1.284825688858352, 0.9324885323123182, 1.4643023219913938, 0.18331771995836993, 1.088103257711666, 0.6416690842580773, 1.0950606626324466, -0.7491226698296581, 1.0441318260534358, -0.008745404082897113, -0.507840883835327, 0.5116985574475971, 0.2649765081120461, 0.9849345104708902, -0.17392537168176844, 0.9278607809442472, 0.6602837616969366, 0.23610499331767118, 0.9447337280198131, 0.04522697629869929, 1.1162831773500494, 0.3578884179973069, 1.2981838753589134, 0.9217289224470369, 0.7148533259114115, 0.9001743731721675, 1.3320244367220888, 1.5578021550660706, 0.8132906498175411, -0.3055837780243097, -0.0681264920683643, -0.1896224723785027, 0.13642871155801442, 1.380026435915511, 0.5880064433398632, 0.8068583125004121, 1.2206908824443528, 0.8150952357665997, 0.24595767988669195, 0.1664132171454563, 0.46480028840377874, 0.8908151984694491, 0.23054802355945483, -0.7045317592446648, 0.35698149470104656, 0.3319344799450578, 0.37621565106666993, 0.6425760075543377, -0.9064698649488373, 0.2990008418781428], "y": [0.6851604704673088, 0.3183336382626288, 0.5042628153092042, -0.8279590118206325, 0.07459067519771612, -0.41824687156867923, 0.28346827006152875, -1.0048909611818957, 0.2283618997883956, -0.7230819050048347, -1.2659711905263942, -0.10398123549904077, -0.5500364636804748, -0.12481785412635765, -0.2548542087025897, 0.46717949444151063, -0.352969665723851, -0.3336107668249156, -0.5431455507797667, -0.5838343774718646, -0.08461685219478861, -0.06892503165601409, -0.3277873083339176, -0.1827377936213677, 0.14905594436978475, 0.3285044738343233, 0.24444087601634362, 0.2674954473102546, -0.1633503006876166, -0.36826218975458813, -0.7051721317994657, -0.6802867635281337, -0.31403243824923693, -0.42095428731388235, -0.48428741998121894, 0.19418231471315045, 0.40761959361100714, -0.3720370599095018, -0.2685243966220153, -0.6819267248636272, -0.6707115445117209, -0.03446444436826912, -0.40438584800732535, -1.0122482275317148, -0.504910093337109, -0.21265468378117022, -0.29321892925141935, 0.01773819011241614, -0.7560933665990144, -0.3488978064503363], "mode": "markers", "name": "versicolor", "marker": {"color": "rgba(55, 128, 191, 1.0)", "symbol": "dot", "size": 12, "opacity": 0.8, "line": {"width": 1.3}}, "textfont": {"color": "#4D5663"}}, {"type": "scatter", "x": [2.531192727803628, 1.4152358767039022, 2.61667601599569, 1.9715310530434356, 2.3500059200446404, 3.3970387360532595, 0.521232243909773, 2.9325870689936897, 2.3212288165733783, 2.9167509667860734, 1.6617741536365316, 1.803401952965091, 2.1655917960801454, 1.3461635794584512, 1.5859282238732209, 1.904456374793427, 1.9496890593990694, 3.4870553642902804, 3.7956454220728846, 1.300791712637657, 2.427817913066046, 1.19900110546556, 3.4999200389245386, 1.3887661316914652, 2.275430503872205, 2.614090473810832, 1.2585081605114878, 1.2911320591150208, 2.123608722773895, 2.3880030160034686, 2.8416727781038715, 3.230673661432094, 2.15943764248905, 1.4441612423295096, 1.7812948100451123, 3.076499931687188, 2.144243314302082, 1.9050981488140755, 1.1693263393414999, 2.107611143257242, 2.3141547052356, 1.9222678009026013, 1.4152358767039022, 2.5630133750774764, 2.4187461827328254, 1.944109794546968, 1.527166614814517, 1.7643457170444292, 1.9009416142184234, 1.3901888619479135], "y": [-0.009849109498802801, -0.5749163475464901, 0.3439031513417341, -0.17972790435224595, -0.0402609471425316, 0.5508366730280554, -1.1927587270006463, 0.3555000029774963, -0.24383150231069087, 0.7827919488152781, 0.2422284077550667, -0.21563761733355577, 0.21627558507402442, -0.7768183473443404, -0.5396407140267195, 0.11925069209197217, 0.041943259663211024, 1.1757393297134293, 0.2573229734204794, -0.7611496364350637, 0.3781960126170504, -0.6060915277579313, 0.46067409891189504, -0.20439932735215124, 0.334990605821677, 0.5609013551230777, -0.17970479472274706, -0.1166686511740119, -0.2097294766773032, 0.46463980470873645, 0.3752691671951033, 1.3741650867930475, -0.21727757866904956, -0.143413410457581, -0.499901681078137, 0.6880856775711758, 0.1400642010897892, 0.04930052601302983, -0.16499026202311012, 0.3722878719607974, 0.18365127916901844, 0.4092034668160619, -0.5749163475464901, 0.2778626029291944, 0.3047981978546915, 0.18753230280060482, -0.3753169825804888, 0.07885885451847535, 0.11662795851202283, -0.28266093799055114], "mode": "markers", "name": "virginica", "marker": {"color": "rgba(50, 171, 96, 1.0)", "symbol": "dot", "size": 12, "opacity": 0.8, "line": {"width": 1.3}}, "textfont": {"color": "#4D5663"}}], {"legend": {"bgcolor": "#F5F6F9", "font": {"color": "#4D5663"}}, "paper_bgcolor": "#F5F6F9", "plot_bgcolor": "#F5F6F9", "yaxis1": {"tickfont": {"color": "#4D5663"}, "gridcolor": "#E1E5ED", "titlefont": {"color": "#4D5663"}, "zerolinecolor": "#E1E5ED", "showgrid": true, "title": ""}, "xaxis1": {"tickfont": {"color": "#4D5663"}, "gridcolor": "#E1E5ED", "titlefont": {"color": "#4D5663"}, "zerolinecolor": "#E1E5ED", "showgrid": true, "title": ""}, "titlefont": {"color": "#4D5663"}}, {"showLink": true, "linkText": "Export to plot.ly"})});</script></div></div>
</div>
<p>We will work only with one class for now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CLASS</span> <span class="o">=</span> <span class="s1">&#39;versicolor&#39;</span>

<span class="c1"># Prepare data for 2 class test</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">CLASS</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">]</span>

<span class="c1"># Apply standart scaler</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">())</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Add ones for intercept</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>

<span class="c1"># Make mask for tain/test set</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">0.7</span>

<span class="n">N</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<div class="section" id="perceptron-for-2-class-case">
<h3>Perceptron for 2 class case<a class="headerlink" href="#perceptron-for-2-class-case" title="Permalink to this headline">¶</a></h3>
<p>We will produce following perceptron from scratch</p>
<img src="https://miro.medium.com/max/2870/1*n6sJ4yZQzwKL9wnF5wnVNg.png" style="width: 60%"/><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># Initial weights between 0 and 1</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Train only on train set</span>
    <span class="k">for</span> <span class="n">features</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">]):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>     <span class="c1"># step function</span>
        <span class="n">W</span> <span class="o">+=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">label</span> <span class="o">-</span> <span class="n">pred</span><span class="p">)</span> <span class="o">*</span> <span class="n">features</span>

<span class="n">pred_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">W</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">pred_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">W</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Hit rate (train set) - </span><span class="si">{0:.02%}</span><span class="s1">, Hit rate (test set) - </span><span class="si">{1:.02%}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="p">(</span><span class="n">pred_train</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="p">(</span><span class="n">pred_test</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Hit rate (train set) - 63.81%, Hit rate (test set) - 66.67%
</pre></div>
</div>
</div>
</div>
<p>Interestingly Alexey Grigorevich Ivakhnenko in 1965 introduced first deep neural network, but it was forgotten…</p>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571319956543_image.png" alt="First known deep network" style="width: 50%"/></div>
</div>
<div class="section" id="beginning-of-first-ai-winter">
<h2>Beginning of first AI winter<a class="headerlink" href="#beginning-of-first-ai-winter" title="Permalink to this headline">¶</a></h2>
<p>In their <strong>1969</strong> monograph <em>Perceptrons</em>, Marvin Minsky and Seymour <strong>Papert</strong> highlighted a number of serious weaknesses of Perceptrons—in particular, the fact that they are incapable of solving some trivial problems (e.g., the <em>Exclusive OR</em> (XOR) classification problem.
It turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons!</p>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571117686838_image.png"  style="width: 50%"/><p>But, we have no idea how to train it yet…</p>
<img src="https://miro.medium.com/max/2101/1*mWYZanOv3QUafz0nnhbEWw.png"  style="width: 50%"/></div>
<div class="section" id="backpropagation">
<h2>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">¶</a></h2>
<p>When an ANN contains a deep stack of hidden layers, it is called a <em><strong>deep neural network</strong></em> <strong>(DNN)</strong>.</p>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571117766740_image.png" alt="Architecture of a Multilayer Perceptron" style="width: 50%"/><p>For many years researchers struggled to find a way to train MLPs, without success. But in <strong>1986</strong>, David <strong>Rumelhart</strong>, Geoffrey <strong>Hinton</strong>, and Ronald <strong>Williams</strong> published a groundbreaking paper (“Learning Internal Representations by Error Propagation”) that introduced the <strong>backpropagation</strong> training algorithm, which is still used today.</p>
<p>Actually backpropagation was known prior to 1986 (Paul Werbos, 1975)</p>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571320041201_image.png" alt="Paul Werbos, 1975" style="width: 50%"/><p>In short, it is Gradient Descent using an efficient technique for computing the gradients automatically: in just <strong>two passes</strong> through the network (one <strong>forward</strong>, one <strong>backward</strong>), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter.</p>
<img src="https://miro.medium.com/max/3040/1*q1M7LGiDTirwU-4LcFq7_Q.png" style="width: 50%"/><p>Automatically computing gradients is called <em>automatic differentiation</em>, or <em>autodiff</em>. There are various autodiff techniques, with different pros and cons. The one used by backpropagation is called <em>reverse-mode autodiff</em>.</p>
<div class="section" id="two-layer-nn-for-2-class-case">
<h3>Two layer NN for 2 class case<a class="headerlink" href="#two-layer-nn-for-2-class-case" title="Permalink to this headline">¶</a></h3>
<img src="https://miro.medium.com/max/1000/1*sX6T0Y4aa3ARh7IBS_sdqw.png" style="width: 40%"/><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="n">diff</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="n">hidden</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Initial weights between 0 and 1</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">W_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">hidden</span><span class="p">))</span>
<span class="n">W_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p>Following code implements <strong>back propagation</strong> using gradient descent.</p>
<img src="https://kratzert.github.io/images/bn_backpass/chainrule_example.PNG" style="width: 80%"/><p>For extensive chain rule back propagation explanation see <a class="reference external" href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">this blog post</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Forward pass (make prediction)</span>
    <span class="n">L_1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_0</span><span class="p">))</span>
    <span class="n">L_2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">L_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="p">))</span>
    <span class="c1"># Backward pass (propagate diff)</span>
    <span class="n">diff_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">-</span> <span class="n">L_2</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span><span class="p">(</span><span class="n">L_2</span><span class="p">)</span>
    <span class="n">diff_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff_2</span><span class="p">,</span> <span class="n">W_1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span><span class="p">(</span><span class="n">L_1</span><span class="p">)</span>
    <span class="n">W_1</span> <span class="o">+=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L_1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">diff_2</span><span class="p">)</span>
    <span class="n">W_0</span> <span class="o">+=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">diff_1</span><span class="p">)</span>

<span class="n">pred_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_0</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">pred_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_0</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W_1</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Hit rate (train set) - </span><span class="si">{0:.02%}</span><span class="s1">, Hit rate (test set) - </span><span class="si">{1:.02%}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="p">(</span><span class="n">pred_train</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="p">(</span><span class="n">pred_test</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Hit rate (train set) - 80.00%, Hit rate (test set) - 71.11%
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="keras">
<h2>Keras<a class="headerlink" href="#keras" title="Permalink to this headline">¶</a></h2>
<p>From now on we will use keras to create our models. We will use relu activation</p>
<img src="https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png" style="width: 30%"/><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">hist</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_26 (Dense)             (None, 4)                 24        
_________________________________________________________________
dense_27 (Dense)             (None, 1)                 5         
=================================================================
Total params: 29
Trainable params: 29
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
105/105 [==============================] - 1s 6ms/step - loss: 0.8314
Epoch 2/100
105/105 [==============================] - 0s 83us/step - loss: 0.7628
Epoch 3/100
105/105 [==============================] - 0s 83us/step - loss: 0.7001
Epoch 4/100
105/105 [==============================] - 0s 95us/step - loss: 0.6404
Epoch 5/100
105/105 [==============================] - 0s 90us/step - loss: 0.5924
Epoch 6/100
105/105 [==============================] - 0s 92us/step - loss: 0.5480
Epoch 7/100
105/105 [==============================] - 0s 124us/step - loss: 0.5121
Epoch 8/100
105/105 [==============================] - 0s 161us/step - loss: 0.4778
Epoch 9/100
105/105 [==============================] - 0s 79us/step - loss: 0.4506
Epoch 10/100
105/105 [==============================] - 0s 126us/step - loss: 0.4271
Epoch 11/100
105/105 [==============================] - 0s 104us/step - loss: 0.4043
Epoch 12/100
105/105 [==============================] - 0s 75us/step - loss: 0.3824
Epoch 13/100
105/105 [==============================] - 0s 129us/step - loss: 0.3630
Epoch 14/100
105/105 [==============================] - 0s 126us/step - loss: 0.3446
Epoch 15/100
105/105 [==============================] - 0s 122us/step - loss: 0.3281
Epoch 16/100
105/105 [==============================] - 0s 114us/step - loss: 0.3137
Epoch 17/100
105/105 [==============================] - 0s 91us/step - loss: 0.3030
Epoch 18/100
105/105 [==============================] - 0s 104us/step - loss: 0.2920
Epoch 19/100
105/105 [==============================] - 0s 130us/step - loss: 0.2826
Epoch 20/100
105/105 [==============================] - 0s 130us/step - loss: 0.2737
Epoch 21/100
105/105 [==============================] - 0s 133us/step - loss: 0.2669
Epoch 22/100
105/105 [==============================] - 0s 134us/step - loss: 0.2603
Epoch 23/100
105/105 [==============================] - 0s 174us/step - loss: 0.2537
Epoch 24/100
105/105 [==============================] - 0s 160us/step - loss: 0.2478
Epoch 25/100
105/105 [==============================] - 0s 194us/step - loss: 0.2408
Epoch 26/100
105/105 [==============================] - 0s 218us/step - loss: 0.2347
Epoch 27/100
105/105 [==============================] - 0s 163us/step - loss: 0.2293
Epoch 28/100
105/105 [==============================] - 0s 153us/step - loss: 0.2238
Epoch 29/100
105/105 [==============================] - 0s 157us/step - loss: 0.2187
Epoch 30/100
105/105 [==============================] - 0s 183us/step - loss: 0.2125
Epoch 31/100
105/105 [==============================] - 0s 202us/step - loss: 0.2065
Epoch 32/100
105/105 [==============================] - 0s 181us/step - loss: 0.2010
Epoch 33/100
105/105 [==============================] - 0s 175us/step - loss: 0.1959
Epoch 34/100
105/105 [==============================] - 0s 173us/step - loss: 0.1905
Epoch 35/100
105/105 [==============================] - 0s 147us/step - loss: 0.1848
Epoch 36/100
105/105 [==============================] - 0s 165us/step - loss: 0.1796
Epoch 37/100
105/105 [==============================] - 0s 168us/step - loss: 0.1749
Epoch 38/100
105/105 [==============================] - 0s 211us/step - loss: 0.1697
Epoch 39/100
105/105 [==============================] - 0s 173us/step - loss: 0.1646
Epoch 40/100
105/105 [==============================] - 0s 181us/step - loss: 0.1598
Epoch 41/100
105/105 [==============================] - 0s 189us/step - loss: 0.1546
Epoch 42/100
105/105 [==============================] - 0s 176us/step - loss: 0.1500
Epoch 43/100
105/105 [==============================] - 0s 159us/step - loss: 0.1461
Epoch 44/100
105/105 [==============================] - 0s 125us/step - loss: 0.1424
Epoch 45/100
105/105 [==============================] - 0s 177us/step - loss: 0.1379
Epoch 46/100
105/105 [==============================] - 0s 169us/step - loss: 0.1347
Epoch 47/100
105/105 [==============================] - 0s 140us/step - loss: 0.1312
Epoch 48/100
105/105 [==============================] - 0s 145us/step - loss: 0.1293
Epoch 49/100
105/105 [==============================] - 0s 159us/step - loss: 0.1269
Epoch 50/100
105/105 [==============================] - 0s 147us/step - loss: 0.1234
Epoch 51/100
105/105 [==============================] - 0s 106us/step - loss: 0.1191
Epoch 52/100
105/105 [==============================] - 0s 117us/step - loss: 0.1168
Epoch 53/100
105/105 [==============================] - 0s 71us/step - loss: 0.1144
Epoch 54/100
105/105 [==============================] - 0s 103us/step - loss: 0.1113
Epoch 55/100
105/105 [==============================] - 0s 106us/step - loss: 0.1094
Epoch 56/100
105/105 [==============================] - 0s 74us/step - loss: 0.1073
Epoch 57/100
105/105 [==============================] - 0s 65us/step - loss: 0.1053
Epoch 58/100
105/105 [==============================] - 0s 64us/step - loss: 0.1032
Epoch 59/100
105/105 [==============================] - 0s 64us/step - loss: 0.1015
Epoch 60/100
105/105 [==============================] - 0s 70us/step - loss: 0.0995
Epoch 61/100
105/105 [==============================] - 0s 109us/step - loss: 0.0979
Epoch 62/100
105/105 [==============================] - 0s 63us/step - loss: 0.0965
Epoch 63/100
105/105 [==============================] - 0s 93us/step - loss: 0.0945
Epoch 64/100
105/105 [==============================] - 0s 94us/step - loss: 0.0931
Epoch 65/100
105/105 [==============================] - 0s 176us/step - loss: 0.0918
Epoch 66/100
105/105 [==============================] - 0s 136us/step - loss: 0.0909
Epoch 67/100
105/105 [==============================] - 0s 187us/step - loss: 0.0895
Epoch 68/100
105/105 [==============================] - 0s 115us/step - loss: 0.0883
Epoch 69/100
105/105 [==============================] - 0s 117us/step - loss: 0.0871
Epoch 70/100
105/105 [==============================] - 0s 98us/step - loss: 0.0865
Epoch 71/100
105/105 [==============================] - 0s 83us/step - loss: 0.0850
Epoch 72/100
105/105 [==============================] - 0s 102us/step - loss: 0.0848
Epoch 73/100
105/105 [==============================] - 0s 111us/step - loss: 0.0829
Epoch 74/100
105/105 [==============================] - 0s 89us/step - loss: 0.0818
Epoch 75/100
105/105 [==============================] - 0s 130us/step - loss: 0.0820
Epoch 76/100
105/105 [==============================] - 0s 120us/step - loss: 0.0811
Epoch 77/100
105/105 [==============================] - 0s 111us/step - loss: 0.0805
Epoch 78/100
105/105 [==============================] - 0s 84us/step - loss: 0.0788
Epoch 79/100
105/105 [==============================] - 0s 143us/step - loss: 0.0776
Epoch 80/100
105/105 [==============================] - 0s 151us/step - loss: 0.0770
Epoch 81/100
105/105 [==============================] - 0s 122us/step - loss: 0.0764
Epoch 82/100
105/105 [==============================] - 0s 114us/step - loss: 0.0755
Epoch 83/100
105/105 [==============================] - 0s 144us/step - loss: 0.0749
Epoch 84/100
105/105 [==============================] - 0s 97us/step - loss: 0.0752
Epoch 85/100
105/105 [==============================] - 0s 102us/step - loss: 0.0735
Epoch 86/100
105/105 [==============================] - 0s 99us/step - loss: 0.0741
Epoch 87/100
105/105 [==============================] - 0s 84us/step - loss: 0.0728
Epoch 88/100
105/105 [==============================] - 0s 88us/step - loss: 0.0728
Epoch 89/100
105/105 [==============================] - 0s 142us/step - loss: 0.0725
Epoch 90/100
105/105 [==============================] - 0s 105us/step - loss: 0.0720
Epoch 91/100
105/105 [==============================] - 0s 96us/step - loss: 0.0708
Epoch 92/100
105/105 [==============================] - 0s 77us/step - loss: 0.0707
Epoch 93/100
105/105 [==============================] - 0s 101us/step - loss: 0.0696
Epoch 94/100
105/105 [==============================] - 0s 119us/step - loss: 0.0693
Epoch 95/100
105/105 [==============================] - 0s 69us/step - loss: 0.0689
Epoch 96/100
105/105 [==============================] - 0s 97us/step - loss: 0.0688
Epoch 97/100
105/105 [==============================] - 0s 69us/step - loss: 0.0689
Epoch 98/100
105/105 [==============================] - 0s 85us/step - loss: 0.0684
Epoch 99/100
105/105 [==============================] - 0s 86us/step - loss: 0.0676
Epoch 100/100
105/105 [==============================] - 0s 71us/step - loss: 0.0669
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff192e40c88&gt;
</pre></div>
</div>
<img alt="_images/DNN_17_1.png" src="_images/DNN_17_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pred_train</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Hit rate (train set) - </span><span class="si">{0:.02%}</span><span class="s1">, Hit rate (test set) - </span><span class="si">{1:.02%}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="p">(</span><span class="n">pred_train</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="p">(</span><span class="n">pred_test</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Hit rate (train set) - 97.14%, Hit rate (test set) - 97.78%
</pre></div>
</div>
</div>
</div>
<p>Much better!</p>
</div>
<div class="section" id="mnist-application-of-backprop-by-yann-lecun-1989">
<h2>MNIST (Application of backprop by Yann LeCun, 1989)<a class="headerlink" href="#mnist-application-of-backprop-by-yann-lecun-1989" title="Permalink to this headline">¶</a></h2>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571320140589_image.png" style="width: 50%"/>
<img src="https://camo.githubusercontent.com/807102dc1f1f17a1318535548d05d54867135be2/68747470733a2f2f6d6c34612e6769746875622e696f2f696d616765732f666967757265732f6d6e6973742d696e7075742e706e67" style="width: 50%"/>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571320219769_image.png" alt="Before training" style="width: 50%"/>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571320227765_image.png" alt="After training" style="width: 50%"/>
<img src="https://paper-attachments.dropbox.com/s_1F3ADB50AE0E9BFFE47A31C329F32861D3C6E5DDE635C337B9BB647E093C8832_1571118051404_image.png" style="width: 50%"/><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="c1"># Normalize</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span> <span class="o">/</span> <span class="mi">255</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">6</span><span class="o">-</span><span class="mi">5</span><span class="n">fc97eba29a5</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>     <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>     <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;plt&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;sgd&quot;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 64)                50240     
_________________________________________________________________
dense_1 (Dense)              (None, 32)                2080      
_________________________________________________________________
dense_2 (Dense)              (None, 10)                330       
=================================================================
Total params: 52,650
Trainable params: 52,650
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 48000 samples, validate on 12000 samples
Epoch 1/20
48000/48000 [==============================] - 3s 55us/sample - loss: 0.7528 - accuracy: 0.8008 - val_loss: 0.3494 - val_accuracy: 0.9028
Epoch 2/20
48000/48000 [==============================] - 2s 50us/sample - loss: 0.3325 - accuracy: 0.9053 - val_loss: 0.2806 - val_accuracy: 0.9209
Epoch 3/20
48000/48000 [==============================] - 3s 57us/sample - loss: 0.2765 - accuracy: 0.9198 - val_loss: 0.2482 - val_accuracy: 0.9284
Epoch 4/20
48000/48000 [==============================] - 3s 56us/sample - loss: 0.2416 - accuracy: 0.9299 - val_loss: 0.2202 - val_accuracy: 0.9371
Epoch 5/20
48000/48000 [==============================] - 3s 62us/sample - loss: 0.2155 - accuracy: 0.9377 - val_loss: 0.2028 - val_accuracy: 0.9427
Epoch 6/20
48000/48000 [==============================] - 3s 53us/sample - loss: 0.1945 - accuracy: 0.9426 - val_loss: 0.1897 - val_accuracy: 0.9457
Epoch 7/20
48000/48000 [==============================] - 3s 56us/sample - loss: 0.1765 - accuracy: 0.9488 - val_loss: 0.1711 - val_accuracy: 0.9519
Epoch 8/20
48000/48000 [==============================] - 3s 56us/sample - loss: 0.1624 - accuracy: 0.9524 - val_loss: 0.1626 - val_accuracy: 0.9542
Epoch 9/20
48000/48000 [==============================] - 3s 52us/sample - loss: 0.1500 - accuracy: 0.9561 - val_loss: 0.1535 - val_accuracy: 0.9562
Epoch 10/20
48000/48000 [==============================] - 2s 52us/sample - loss: 0.1393 - accuracy: 0.9598 - val_loss: 0.1446 - val_accuracy: 0.9594
Epoch 11/20
48000/48000 [==============================] - 2s 50us/sample - loss: 0.1291 - accuracy: 0.9626 - val_loss: 0.1419 - val_accuracy: 0.9603
Epoch 12/20
48000/48000 [==============================] - 3s 60us/sample - loss: 0.1215 - accuracy: 0.9645 - val_loss: 0.1341 - val_accuracy: 0.9613
Epoch 13/20
48000/48000 [==============================] - 3s 58us/sample - loss: 0.1141 - accuracy: 0.9674 - val_loss: 0.1304 - val_accuracy: 0.9632
Epoch 14/20
48000/48000 [==============================] - 3s 54us/sample - loss: 0.1077 - accuracy: 0.9686 - val_loss: 0.1258 - val_accuracy: 0.9633
Epoch 15/20
48000/48000 [==============================] - 3s 65us/sample - loss: 0.1018 - accuracy: 0.9704 - val_loss: 0.1240 - val_accuracy: 0.9649
Epoch 16/20
48000/48000 [==============================] - 3s 66us/sample - loss: 0.0965 - accuracy: 0.9723 - val_loss: 0.1195 - val_accuracy: 0.9666
Epoch 17/20
48000/48000 [==============================] - 3s 65us/sample - loss: 0.0916 - accuracy: 0.9737 - val_loss: 0.1206 - val_accuracy: 0.9654
Epoch 18/20
48000/48000 [==============================] - 3s 60us/sample - loss: 0.0869 - accuracy: 0.9753 - val_loss: 0.1163 - val_accuracy: 0.9660
Epoch 19/20
48000/48000 [==============================] - 3s 59us/sample - loss: 0.0827 - accuracy: 0.9762 - val_loss: 0.1114 - val_accuracy: 0.9677
Epoch 20/20
48000/48000 [==============================] - 3s 56us/sample - loss: 0.0783 - accuracy: 0.9777 - val_loss: 0.1154 - val_accuracy: 0.9662
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">history</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14ff84090&gt;
</pre></div>
</div>
<img alt="_images/DNN_25_1.png" src="_images/DNN_25_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy on test set - </span><span class="si">{0:.02%}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Accuracy on test set - 96.92%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">label</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">pred</span> <span class="o">==</span> <span class="n">label</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/DNN_27_0.png" src="_images/DNN_27_0.png" />
</div>
</div>
<p>Let’s look only at misclassifiesd cases</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">label</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">label</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">mask</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/DNN_29_0.png" src="_images/DNN_29_0.png" />
</div>
</div>
<p>As you might remember Random Forest achieved similar accuracy so why do we need NN then? Well, let’s try to increse number of parameters in out network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;sgd&quot;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              785000    
_________________________________________________________________
dense_4 (Dense)              (None, 500)               500500    
_________________________________________________________________
dense_5 (Dense)              (None, 10)                5010      
=================================================================
Total params: 1,290,510
Trainable params: 1,290,510
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 48000 samples, validate on 12000 samples
Epoch 1/20
48000/48000 [==============================] - 7s 152us/sample - loss: 0.6053 - accuracy: 0.8541 - val_loss: 0.3079 - val_accuracy: 0.9150
Epoch 2/20
48000/48000 [==============================] - 9s 189us/sample - loss: 0.2896 - accuracy: 0.9180 - val_loss: 0.2429 - val_accuracy: 0.9322
Epoch 3/20
48000/48000 [==============================] - 8s 162us/sample - loss: 0.2373 - accuracy: 0.9331 - val_loss: 0.2100 - val_accuracy: 0.9415
Epoch 4/20
48000/48000 [==============================] - 8s 173us/sample - loss: 0.2028 - accuracy: 0.9428 - val_loss: 0.1833 - val_accuracy: 0.9504
Epoch 5/20
48000/48000 [==============================] - 10s 205us/sample - loss: 0.1766 - accuracy: 0.9508 - val_loss: 0.1656 - val_accuracy: 0.9545
Epoch 6/20
48000/48000 [==============================] - 9s 181us/sample - loss: 0.1558 - accuracy: 0.9559 - val_loss: 0.1551 - val_accuracy: 0.9567
Epoch 7/20
48000/48000 [==============================] - 7s 151us/sample - loss: 0.1395 - accuracy: 0.9607 - val_loss: 0.1394 - val_accuracy: 0.9605
Epoch 8/20
48000/48000 [==============================] - 8s 163us/sample - loss: 0.1256 - accuracy: 0.9644 - val_loss: 0.1314 - val_accuracy: 0.9631
Epoch 9/20
48000/48000 [==============================] - 8s 169us/sample - loss: 0.1137 - accuracy: 0.9689 - val_loss: 0.1246 - val_accuracy: 0.9655
Epoch 10/20
48000/48000 [==============================] - 8s 161us/sample - loss: 0.1035 - accuracy: 0.9711 - val_loss: 0.1172 - val_accuracy: 0.9668
Epoch 11/20
48000/48000 [==============================] - 8s 159us/sample - loss: 0.0946 - accuracy: 0.9739 - val_loss: 0.1108 - val_accuracy: 0.9687
Epoch 12/20
48000/48000 [==============================] - 8s 162us/sample - loss: 0.0870 - accuracy: 0.9762 - val_loss: 0.1065 - val_accuracy: 0.9701
Epoch 13/20
48000/48000 [==============================] - 8s 160us/sample - loss: 0.0799 - accuracy: 0.9788 - val_loss: 0.1007 - val_accuracy: 0.9705
Epoch 14/20
48000/48000 [==============================] - 8s 164us/sample - loss: 0.0737 - accuracy: 0.9810 - val_loss: 0.0990 - val_accuracy: 0.9717
Epoch 15/20
48000/48000 [==============================] - 8s 159us/sample - loss: 0.0681 - accuracy: 0.9822 - val_loss: 0.0950 - val_accuracy: 0.9732
Epoch 16/20
48000/48000 [==============================] - 8s 161us/sample - loss: 0.0631 - accuracy: 0.9835 - val_loss: 0.0917 - val_accuracy: 0.9736
Epoch 17/20
48000/48000 [==============================] - 9s 183us/sample - loss: 0.0586 - accuracy: 0.9849 - val_loss: 0.0912 - val_accuracy: 0.9733
Epoch 18/20
48000/48000 [==============================] - 8s 161us/sample - loss: 0.0542 - accuracy: 0.9864 - val_loss: 0.0901 - val_accuracy: 0.9736
Epoch 19/20
48000/48000 [==============================] - 8s 158us/sample - loss: 0.0505 - accuracy: 0.9870 - val_loss: 0.0866 - val_accuracy: 0.9746
Epoch 20/20
48000/48000 [==============================] - 8s 165us/sample - loss: 0.0472 - accuracy: 0.9882 - val_loss: 0.0841 - val_accuracy: 0.9753
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">history</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy on test set - </span><span class="si">{0:.02%}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Accuracy on test set - 97.59%
</pre></div>
</div>
<img alt="_images/DNN_33_1.png" src="_images/DNN_33_1.png" />
</div>
</div>
<p>Thats good. For examply by using layers 1500, 1000, 500, 10 you can improve accuracy to 98.24%, but training will take quite some time. Acctually you can go even futher with simple data augmentation techniques - https://arxiv.org/pdf/1003.0358.pdf</p>
</div>
<div class="section" id="a-note-on-gradien-descent">
<h2>A note on Gradien Descent<a class="headerlink" href="#a-note-on-gradien-descent" title="Permalink to this headline">¶</a></h2>
<p>Without noticing on the way we have used gradient descent, let’s see how it works in depth.</p>
<img src="https://miro.medium.com/max/1005/1*_6TVU8yGpXNYDkkpOfnJ6Q.png" style="width: 50%"/><p>Note, that gradient descent is sensitive to the step size.</p>
<img src="https://www.researchgate.net/profile/Tom_Duckett/publication/224324276/figure/fig2/AS:359779089305617@1462789427971/Convergence-Conditions-in-Gradient-Descent-Algorithm.png" style="width: 50%"/><p>We will meet some solutions to this problem in the future.</p>
<p>Also gradient descent is not guaranteed to find global minimum, but when we work with high dimensional data this risk partly wanishes.</p>
<img src="https://paper-attachments.dropbox.com/s_F57E27FDF0C54777F2844EECCBABB7DF8EEB9597E5F323F9CC73F1690617FCAD_1569311505423_image.png" style="width: 50%"/><p>Let’s implement gradient descent for linear regression as follows</p>
<img src="https://kousikk.files.wordpress.com/2014/11/screen-shot-2014-11-12-at-11-57-47-am.png" style="width: 50%"/><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">k</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>     <span class="c1"># initial values</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">L</span> <span class="o">=</span> <span class="p">[]</span>   <span class="c1"># storage for k and b through training</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">L</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">k</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
    <span class="n">pred_diff_k</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">pred_diff_b</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">k</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pred_diff_k</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pred_diff_b</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>TASK:</strong> try that out on your own and add plots on the way, you should get something similar to this</p>
<img src="https://paper-attachments.dropbox.com/s_F57E27FDF0C54777F2844EECCBABB7DF8EEB9597E5F323F9CC73F1690617FCAD_1567686156887_grad_desc_demo.gif" style="width: 50%"/></div>
<div class="section" id="a-note-on-dropout">
<h2>A note on Dropout<a class="headerlink" href="#a-note-on-dropout" title="Permalink to this headline">¶</a></h2>
<p>There is a nice and popular way to <strong>prevent over-fitting</strong> in Neural Networks - turn off some neurons during training procudure.</p>
<img src="https://miro.medium.com/proxy/1*iWQzxhVlvadk6VAJjsgXgg.png" style="width: 50%"/><p>You can try that out by adding Dropout layers between Dense ones as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>Try that out.</p>
</div>
<div class="section" id="a-note-on-architectures">
<h2>A note on Architectures<a class="headerlink" href="#a-note-on-architectures" title="Permalink to this headline">¶</a></h2>
<p>There are a lot of possible architectures, we have only touched the surface. Take a glympse at the variations of Neural Networks <a class="reference external" href="https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464">here</a>.</p>
<img src="https://miro.medium.com/max/2000/1*cuTSPlTq0a_327iTPJyD-Q.png" style="width: 80%"/></div>
</div>
<div class="section" id="re-sources">
<h1>(re)Sources<a class="headerlink" href="#re-sources" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Rosenblatt’s perceptron, the first modern neural network (<a class="reference external" href="https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a">blog post</a>)</p></li>
<li><p>Neural Network in 11 lines of code (<a class="reference external" href="https://iamtrask.github.io/2015/07/12/basic-python-network/">blog post</a>)</p></li>
<li><p>Favio Vázquez <a class="reference external" href="https://medium.com/&#64;faviovazquez">posts</a></p></li>
<li><p>Neural Networks with good MNIST <a class="reference external" href="https://ml4a.github.io/ml4a/neural_networks/">demo</a></p></li>
<li><p>Legendary Andrew Ng <a class="reference external" href="https://www.coursera.org/learn/machine-learning">course</a></p></li>
</ul>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="SVD.html" title="previous page">SVD (Singular Value Decomposition)</a>

    </div>
    <footer class="footer">
  <div class="container">
    <p>
          &copy; Copyright MIF, 2020.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.4.4.<br/>
    </p>
  </div>
</footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>