{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f314f8d9",
   "metadata": {},
   "source": [
    "# Namesformer\n",
    "\n",
    "Inspired by Andrej Karpathy lecture [makemore](https://www.youtube.com/watch?v=PaCmpygFfXo&t=131s) that contains english name generation. \n",
    "\n",
    "The code was fully writen using ChatGPT with minimal corrections. My first query was:\n",
    "\n",
    "```\n",
    "I am preparing a lecture for my students on AI basics. They already know how to use attention in keras to create self-attention layers. What I want to explain them is how to make a simplest possible transformer architecture (with minimal amount of code).\n",
    " As a dataset I will use a csv with names:\n",
    "    john\n",
    "    peter\n",
    "    mike\n",
    "    ...\n",
    "And the goal will be to generate more names that sound name-like.\n",
    "Give me an implementation with keras trying to keep it as minimal as possible.\n",
    "```\n",
    "\n",
    "After that I had to ask for couple corrections, like avoiding using Transformer layer, adding comments, fixing a bug in token indexing. All were relatively easy to spot and in less than an hour this notebook was generating plausibly sounding names.\n",
    "\n",
    "I decided to replace original dataset since I found a list of Lithuanian names that are easy to extract from [vardai.vlkk.lt](vardai.vlkk.lt) using the following code snippet:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "names = []\n",
    "for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "            'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:\n",
    "    url = f'https://vardai.vlkk.lt/sarasas/{key}/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', class_='names_list__links names_list__links--man')\n",
    "    names += [name.text for name in links]\n",
    "```\n",
    "\n",
    "If you want to play with english names download them from [here](https://github.com/karpathy/makemore/blob/master/names.txt) and use *names.txt* instead of *vardai.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c33fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fd97ee0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ãbas', 'Ãbdijus', 'Abdònas', ..., 'Žilvynas', 'Žimantas',\n",
       "       'Žydrunas'], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.read_csv('vardai.txt')['name'].values\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9ec9c427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3495"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d51df",
   "metadata": {},
   "source": [
    "Let's add a space at the end to mark the end of the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eb821fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "names += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "81f147f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ãbas '"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597d5eb",
   "metadata": {},
   "source": [
    "Note that this dataset is not simple since it uses accentuation symbols and capital letters. Let's intentionally keep it like this and see if the model can figure it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb8d463d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 49, (28867, 16), (28867, 49))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=True, lower=True)\n",
    "tokenizer.fit_on_texts(names)\n",
    "\n",
    "max_len = max([len(name) for name in names])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input and output sequences\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "\n",
    "for name in names:\n",
    "    for i in range(1, len(name)):\n",
    "        input_seq = name[:i]\n",
    "        output_seq = name[i]\n",
    "        input_sequences.append(input_seq)\n",
    "        output_sequences.append(output_seq)\n",
    "\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(input_sequences), maxlen=max_len, padding='post')\n",
    "y = to_categorical(np.array(tokenizer.texts_to_sequences(output_sequences)) - 1, num_classes=vocab_size)\n",
    "\n",
    "max_len, vocab_size, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00df65",
   "metadata": {},
   "source": [
    "Our transformer will be based on the self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "611440fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(Model):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        # Create an Embedding layer for converting input tokens into vectors\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        # Add positional encoding to the input embeddings\n",
    "        self.pos_encoding = self.add_positional_encoding(max_len, d_model)\n",
    "        # Create an Attention layer for self-attention mechanism\n",
    "        self.attention = layers.Attention()\n",
    "        # Create a Flatten layer to flatten the output tensor\n",
    "        self.flatten = layers.Flatten()\n",
    "        # Create a Dense layer for generating output probabilities\n",
    "        self.dense = layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Pass the input through the Embedding layer\n",
    "        x = self.embedding(inputs)\n",
    "        # Scale the embeddings by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding.output_dim, tf.float32))\n",
    "        # Add the positional encoding to the input embeddings\n",
    "        x += self.pos_encoding[:, :x.shape[1], :]\n",
    "        # Apply self-attention using the Attention layer\n",
    "        x = self.attention([x, x])\n",
    "        # Flatten the output tensor\n",
    "        x = self.flatten(x)\n",
    "        # Pass the result through the Dense layer to generate output probabilities\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "    def add_positional_encoding(self, max_len, d_model):\n",
    "        # Initialize a positional encoding matrix with zeros\n",
    "        pos_encoding = np.zeros((1, max_len, d_model))\n",
    "        # Calculate the positional encoding values for each position and dimension\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pos_encoding[:, pos, i] = np.sin(pos / np.power(10000, (2 * i) / d_model))\n",
    "                pos_encoding[:, pos, i + 1] = np.cos(pos / np.power(10000, (2 * (i + 1)) / d_model))\n",
    "        # Convert the numpy array to a TensorFlow tensor\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feaf7fd",
   "metadata": {},
   "source": [
    "First we can train for couple epochs just to make sure that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a553610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 20:16:47.818607: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-07 20:16:47.818744: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-05-07 20:16:47.946218: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-07 20:16:48.272305: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903/903 [==============================] - 13s 11ms/step - loss: 2.1334 - accuracy: 0.3874\n",
      "Epoch 2/2\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.7163 - accuracy: 0.4779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291cf7a00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model = SimpleTransformer(vocab_size, d_model=64, max_len=max_len)\n",
    "model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902915ea",
   "metadata": {},
   "source": [
    "And generate a name by predicing the next letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1430820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new names\n",
    "def generate_name(model, seed, maxlen):\n",
    "    generated_name = seed\n",
    "    for _ in range(maxlen):\n",
    "        sequence = tokenizer.texts_to_sequences([generated_name])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "        prediction = model.predict(padded_sequence)[0]\n",
    "        next_char = tokenizer.index_word[np.argmax(prediction) + 1]\n",
    "        if next_char == ' ':\n",
    "            break\n",
    "        generated_name += next_char\n",
    "    return generated_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e7011e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Generated name: Raũdvydas\n"
     ]
    }
   ],
   "source": [
    "seed = 'R'\n",
    "generated_name = generate_name(model, seed, max_len)\n",
    "print(f'Generated name: {generated_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b6a045",
   "metadata": {},
   "source": [
    "Not bad! Note that this name is not in our names list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "424c0507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Raũdvydas ' in names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2c232",
   "metadata": {},
   "source": [
    "Let's train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f956f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.6320 - accuracy: 0.4956\n",
      "Epoch 2/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5971 - accuracy: 0.5021\n",
      "Epoch 3/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5763 - accuracy: 0.5087\n",
      "Epoch 4/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5629 - accuracy: 0.5101\n",
      "Epoch 5/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5512 - accuracy: 0.5087\n",
      "Epoch 6/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5421 - accuracy: 0.5123\n",
      "Epoch 7/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5318 - accuracy: 0.5163\n",
      "Epoch 8/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5226 - accuracy: 0.5161\n",
      "Epoch 9/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5179 - accuracy: 0.5179\n",
      "Epoch 10/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5087 - accuracy: 0.5225\n",
      "Epoch 11/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.5068 - accuracy: 0.5209\n",
      "Epoch 12/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.4960 - accuracy: 0.5251\n",
      "Epoch 13/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.4954 - accuracy: 0.5222\n",
      "Epoch 14/20\n",
      "903/903 [==============================] - 9s 11ms/step - loss: 1.4919 - accuracy: 0.5195\n",
      "Epoch 15/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.4852 - accuracy: 0.5245\n",
      "Epoch 16/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.4842 - accuracy: 0.5230\n",
      "Epoch 17/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.4777 - accuracy: 0.5275\n",
      "Epoch 18/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.4731 - accuracy: 0.5282\n",
      "Epoch 19/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.4740 - accuracy: 0.5258\n",
      "Epoch 20/20\n",
      "903/903 [==============================] - 10s 11ms/step - loss: 1.4709 - accuracy: 0.5262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29dbd5420>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "533f9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Generated name: Rãdmantas\n"
     ]
    }
   ],
   "source": [
    "seed = 'R'\n",
    "generated_name = generate_name(model, seed, max_len)\n",
    "print(f'Generated name: {generated_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f9ab4e",
   "metadata": {},
   "source": [
    "If we want the model to be more creative we can add temperature/creativity control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0584dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model, seed, maxlen, temperature=1.0):\n",
    "    generated_name = seed\n",
    "    for _ in range(maxlen):\n",
    "        sequence = tokenizer.texts_to_sequences([generated_name])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "        prediction = model.predict(padded_sequence)\n",
    "\n",
    "        # Apply temperature to the probability distribution\n",
    "        prediction = np.asarray(prediction).astype('float64')\n",
    "        prediction = np.log(prediction) / temperature\n",
    "        exp_preds = np.exp(prediction)\n",
    "        prediction = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        # Sample the next character from the adjusted probability distribution\n",
    "        next_char_idx = np.argmax(np.random.multinomial(1, prediction.ravel(), 1))\n",
    "        next_char = tokenizer.index_word[next_char_idx + 1]\n",
    "\n",
    "        if next_char == ' ':\n",
    "            break\n",
    "        generated_name += next_char\n",
    "    return generated_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "09461e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Generated name: Rydẽnis\n",
      "Name is new? True\n"
     ]
    }
   ],
   "source": [
    "seed = 'R'\n",
    "generated_name = generate_name(model, seed, max_len)\n",
    "print(f'Generated name: {generated_name}')\n",
    "print(f'Name is new? {generated_name + \" \" not in names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f5db451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Generated name: Rìdmantas\n",
      "Name is new? True\n"
     ]
    }
   ],
   "source": [
    "seed = 'R'\n",
    "generated_name = generate_name(model, seed, max_len)\n",
    "print(f'Generated name: {generated_name}')\n",
    "print(f'Name is new? {generated_name + \" \" not in names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5d87f12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Generated name: Tèlius\n",
      "Name is new? True\n"
     ]
    }
   ],
   "source": [
    "seed = 'T'\n",
    "generated_name = generate_name(model, seed, max_len)\n",
    "print(f'Generated name: {generated_name}')\n",
    "print(f'Name is new? {generated_name + \" \" not in names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "71df699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Generated name: Taũgis\n",
      "Name is new? True\n"
     ]
    }
   ],
   "source": [
    "seed = 'T'\n",
    "generated_name = generate_name(model, seed, max_len)\n",
    "print(f'Generated name: {generated_name}')\n",
    "print(f'Name is new? {generated_name + \" \" not in names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab4baf7",
   "metadata": {},
   "source": [
    "Here we go, we have a Lithuanian name generator!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
