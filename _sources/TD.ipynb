{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# TD (Temporal Difference)\n",
    "\n",
    "Our goal in this lecture is to construct a simple reinforcement learning agent while going through the most important concepts (temporal difference, eligibility traces, ...). On the way we will look for inspiration in behavioristic and neuroscience research and will try to construct our agent from scratch.\n",
    "\n",
    "We start with Pavlov's dog experiment. Most of you know about stimulus reward results - you ring a bell and the dog starts to excrete saliva knowing that food is on the way. That will be the first thing we expect from our system - learn to predict that reward is coming given a stimulus. Since in the end we want to use neural nets let's stick to weight update rule that we have already seen.\n",
    "\n",
    "$$ w_{t+1} = w_{t} + \\text{learning rate} \\cdot \\text{error} \\cdot \\text{diff for given input} $$\n",
    "\n",
    "For simplicity now let's ignore time and focus on single trial as the input. In such a case our input is just a vector of indicators which show the presence of the various stimulus and output is predicted reward. For error we can simply pass difference between our prediction and reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Let's start with simple experiment that contains three trials and where stimulus B clearly predicts reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "#           stimula  reward\n",
    "#          [A, B, C] \n",
    "trials = [[[0, 1, 0], 1],\n",
    "          [[0, 0, 0], 0],\n",
    "          [[0, 1, 0], 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "For the model we will use simplest thing we can think of - weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "w = np.array([0., 0., 0.])    # That's our 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Since we have only three trials we will repeatedly use them for weight updates to get convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.98847078, 0.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(trials, w, alpha=0.2, repeat=10):\n",
    "\n",
    "    for v, r in trials * repeat:    # instead of * we could use second for loop\n",
    "        pred = w.dot(v)\n",
    "        error = r - pred\n",
    "        w += alpha * error * np.array(v)\n",
    "\n",
    "    return w\n",
    "\n",
    "train_model(trials, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "As expected our model learned to associate stimulus B with reward. But our story only begins here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Blocking\n",
    "\n",
    "In his later experiments Pavlov noticed that there is phenomenon called *blocking*. If you teach your dog to associate a bell with food and then introduce something new (for example showing some picture) after a bell - the dog will not learn to associate this new stimulus with food. Let's test if our model does that. If you have executed cells above you already have trained weights and we can introduce some new trials that try to show stimulus C. This new stimulus should be ignored according to the experimental evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "#           stimula  reward\n",
    "#          [A, B, C] \n",
    "trials = [[[0, 1, 1], 1],\n",
    "          [[0, 0, 0], 0],\n",
    "          [[0, 1, 1], 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.99423518, 0.0057644 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(trials, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Since model uses errors for the update if we have a stimulus that is able to predict reward it will block further learning. This is actually known as Rescota-Wagner model and in Barto & Sutton [book](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf) is expressed as follows.\n",
    "\n",
    "For the prediction using weights we have a dot product\n",
    "\n",
    "$$\\hat{v}(s, w) = w^T x(s).$$\n",
    "\n",
    "Then our error is defined as\n",
    "\n",
    "$$\\delta_t = R_t - \\hat{v}(S_t, w_t).$$\n",
    "\n",
    "As implemented above update rule with learning rate $\\alpha$ is\n",
    "\n",
    "$$w_{t+1} = w_t + \\alpha \\delta_t x(S_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Higher order conditioning\n",
    "\n",
    "Notice that our model does not take time into account. That is a huge limitation. For sure we can not look for stimulus reward response for infinite time and splitting into trials does not represent reality well. It turns out that if after training to associate the bell with food you show an image before the bell, dog learns to associate that with food. Not only that there is effect called *higher order conditioning* and if after introducing new stimulus you do not give reward, dog still will learn to associate it with food. It seams that instead of learning that image associates with reward he learns that image predicts bell and bell predicts reward, thus even if reward is not given he will learn to pass know-how.\n",
    "\n",
    "We will have to introduce time into our model. If you think about experiments described above it makes sense to say that model should not predict reward directly, but instead at each time step focus on difference in it's own predictions. This is called *temporal difference* and we will discuss it thoroughly in the lecture. For the time lag we can simply add some factor that discount previous inputs. This trick is known as *eligibility traces*.\n",
    "\n",
    "It's time to put those ideas to the test. First let's implement those and check if simple conditioning and blocking works as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "#           stimula  reward\n",
    "#          [A, B, C] \n",
    "trials = [[[0, 1, 0], 1],\n",
    "          [[0, 0, 0], 0],\n",
    "          [[0, 1, 0], 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "To simulate time we will run through each stimulus in sequence and accumulate history of stimulus presence (see *v_accum*). Also, note that reward is received only at the end of the trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.98847078, 0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(trials, w, alpha=0.2, repeat=10):\n",
    "    dim = w.shape[0]\n",
    "    for v, r in trials * repeat:\n",
    "        z = np.zeros(dim)\n",
    "        prev_pred = 0\n",
    "        v_accum = np.tri(dim, dim) * v\n",
    "        for t in range(dim):\n",
    "            v_t = v_accum[t]\n",
    "            pred = w.dot(v_t)\n",
    "            if t < 2:\n",
    "                # reward is zero while stimulus are coming in,\n",
    "                # thus we look only for TD error\n",
    "                next_v_t = v_accum[t + 1]\n",
    "                next_pred = w.dot(next_v_t)\n",
    "                error = 0 + next_pred - pred\n",
    "            else:\n",
    "                # At the end there is no next prediction and\n",
    "                # we will finally pass actual reward signal\n",
    "                error = r - pred\n",
    "\n",
    "            w += alpha * error * v_t\n",
    "            prev_pred = pred\n",
    "\n",
    "    return w\n",
    "\n",
    "w = np.array([0., 0., 0.])    # Let's start over\n",
    "train_model(trials, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Let's check if blocking still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 9.99547284e-01, 7.99164294e-04])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#           stimula  reward\n",
    "#          [A, B, C] \n",
    "trials = [[[0, 1, 1], 1],\n",
    "          [[0, 0, 0], 0],\n",
    "          [[0, 1, 1], 1]]\n",
    "\n",
    "train_model(trials, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Higher order conditioning with reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94702972, 0.0944271 , 0.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset to the original state first\n",
    "trials = [[[0, 1, 0], 1],\n",
    "          [[0, 0, 0], 0],\n",
    "          [[0, 1, 0], 1]]\n",
    "\n",
    "w = np.array([0., 0., 0.])\n",
    "train_model(trials, w)\n",
    "\n",
    "#           stimula  reward\n",
    "#          [A, B, C] \n",
    "trials = [[[1, 1, 0], 1],\n",
    "          [[0, 0, 0], 0],\n",
    "          [[1, 1, 0], 1]]\n",
    "\n",
    "train_model(trials, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Even if reward is not there every time, model will still learn to transition the signal complying with real life experiments. For sure at the moment our implementation is not flexible enough. To get more control we will introduce couple parameters to the mix - *decay* and *discount*.\n",
    "\n",
    "Formaly $\\text{TD}(\\lambda)$ is defined by following equations (source: Barto & Sutton [book](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)). \n",
    "\n",
    "Error is\n",
    "\n",
    "$$\\delta_t = R_{t+1} + \\delta \\hat{v} (S_{t+1}, w_t) - \\hat{v}(S_t, w_t).$$\n",
    "\n",
    "Weight update is\n",
    "\n",
    "$$w_{t+1} = w_t + \\alpha \\delta_t z_t,$$\n",
    "\n",
    "where $z_t$ is defined by\n",
    "\n",
    "$$z_{t+1} = \\delta \\lambda z_t + x(S_t).$$\n",
    "\n",
    "Here $\\lambda \\in [0, 1]$ is *eligibility trace decay* parameter and $\\delta \\in [0,1]$ is *discount factor*. We can easily add them to our code by modifying\n",
    "\n",
    "```python\n",
    "error = 0 + discount * next_pred - pred\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```python\n",
    "z = decay * discount * z + v_t\n",
    "w += alpha * error * z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Tic-Tac-Toe\n",
    "\n",
    "Most likely you have heard about the success of reinforcement learning in games likes Go. To implement MuZero from scratch would require quite a lot of tricks and we lack a bunch of theory to do that. Instead we will do similar thing as TD-Backgammon while playing Tic-Tac-Toe. We will train $\\text{TD}(0)$ agent from scratch using self play. \n",
    "\n",
    "Let's create simple tic-tac-toe environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1  0  0]\n",
       " [ 0  1  0]\n",
       " [ 0  0  0]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TicTacToe:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.idx = [[0, 1, 2], [3, 4, 5], [6, 7, 8],  # rows\n",
    "                    [0, 3, 6], [1, 4, 7], [2, 5, 8],  # cols\n",
    "                    [0, 4, 8], [2, 4, 6]]  # diagonals\n",
    "        \n",
    "    def move(self, position):\n",
    "        if position not in self.available_actions():\n",
    "            raise Exception('Move is not allowed')\n",
    "        self.board[position] = self.tic_or_tac\n",
    "        win = self.winner()\n",
    "        draw = not len(self.available_actions())\n",
    "        state = self.state()\n",
    "        self.tic_or_tac *= -1\n",
    "        if win:\n",
    "            return 1, state      # a win\n",
    "        if draw:\n",
    "            return 0.5, state    # a draw\n",
    "        return 0, state\n",
    "        \n",
    "    def available_actions(self):\n",
    "        return np.where(self.board == 0)[0]\n",
    "    \n",
    "    def available_action_states(self):\n",
    "        S, A = [], []\n",
    "        for a in self.available_actions():\n",
    "            s = self.state()\n",
    "            s[0, a] = self.tic_or_tac\n",
    "            S.append(s)\n",
    "            A.append(a)\n",
    "        return np.vstack(S), A\n",
    "    \n",
    "    def winner(self):\n",
    "        return np.any([self.board[i].sum() == 3 * self.tic_or_tac\n",
    "                       for i in self.idx])\n",
    "    \n",
    "    def state(self):\n",
    "        return self.board.reshape((1, 9)).astype('int')\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros(9).astype('int')\n",
    "        self.tic_or_tac = 1   # 1 for tic, -1 for tac\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.board.astype('int').reshape((3, 3)))\n",
    "        \n",
    "ttt = TicTacToe()\n",
    "ttt.move(4)\n",
    "ttt.move(0)\n",
    "ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Let's try to win this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "assert ttt.move(1)[0] == 0\n",
    "assert ttt.move(8)[0] == 0\n",
    "assert ttt.move(7)[0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Make sure you understand how states are expressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1,  0,  0,  0,  1,  0,  0,  0,  0],\n",
       "        [ 0, -1,  0,  0,  1,  0,  0,  0,  0],\n",
       "        [ 0,  0, -1,  0,  1,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, -1,  1,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  1, -1,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  1,  0, -1,  0,  0],\n",
       "        [ 0,  0,  0,  0,  1,  0,  0, -1,  0],\n",
       "        [ 0,  0,  0,  0,  1,  0,  0,  0, -1]]),\n",
       " [0, 1, 2, 3, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "ttt.move(4)\n",
    "ttt.available_action_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Following TD ideas presented above we want to push weights in such a way that previous prediction is closer to the new one, thus\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [V(S_{t+1}) - V(S_t)].$$\n",
    "\n",
    "Training is completely driven by self-play. We also need some way to explore new strategies. For the first try we can simple store values in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Nice [viz](https://jinglescode.github.io/reinforcement-learning-tic-tac-toe/) that behind the scenes uses the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, exploration=0.95, lr=0.5):\n",
    "        self.value_dict = {}\n",
    "        self.state = np.zeros((1, 9))\n",
    "        self.lr = lr\n",
    "        self.exploration = exploration\n",
    "\n",
    "    def move(self, ttt):\n",
    "        if np.random.random() > self.exploration:\n",
    "            a = np.random.choice(ttt.available_actions())\n",
    "            r, new_state = ttt.move(a)\n",
    "            if r:    # game over\n",
    "                return r\n",
    "        else:\n",
    "            # Choose the best action according to the stored values\n",
    "            states, actions = ttt.available_action_states()\n",
    "            values = [self.value_dict.get(str(s.reshape((3, 3)).astype('int')), 0.5) for s in states]\n",
    "            a = actions[np.argmax(values)]\n",
    "            r, new_state = ttt.move(a)\n",
    "            key = str(self.state.reshape((3, 3)).astype('int'))\n",
    "            new_key = str(new_state.reshape((3, 3)).astype('int'))\n",
    "            self.value_dict[key] = self.value_dict.get(key, 0.5) + self.lr * (self.value_dict.get(new_key, 0.5) - self.value_dict.get(key, 0.5))\n",
    "            if r:   # game over\n",
    "                self.value_dict[new_key] = self.value_dict.get(new_key, 0.5) + self.lr * ((r if r == 1 else 0) - self.value_dict.get(new_key, 0.5))\n",
    "                return r\n",
    "        self.state = new_state\n",
    "        return 0.\n",
    "    \n",
    "    def lost(self):\n",
    "        key = str(self.state.reshape((3, 3)).astype('int'))\n",
    "        self.value_dict[key] = self.value_dict.get(key, 0.5) + self.lr * (0 - self.value_dict.get(key, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our agents for 10k games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win or draw rate (last 1000 games) - 100.00%\n",
      "win or draw rate (last 1000 games) - 82.30%\n",
      "win or draw rate (last 1000 games) - 95.30%\n",
      "win or draw rate (last 1000 games) - 93.10%\n",
      "win or draw rate (last 1000 games) - 92.80%\n",
      "win or draw rate (last 1000 games) - 85.30%\n",
      "win or draw rate (last 1000 games) - 93.90%\n",
      "win or draw rate (last 1000 games) - 86.50%\n",
      "win or draw rate (last 1000 games) - 91.40%\n",
      "win or draw rate (last 1000 games) - 81.90%\n",
      "win or draw rate (last 1000 games) - 88.70%\n",
      "CPU times: user 1min, sys: 22.7 ms, total: 1min\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "episodes = 10_001\n",
    "agent_1 = Agent(0.95, 0.5)\n",
    "agent_2 = Agent(0.95, 0.5)\n",
    "\n",
    "win_history = []\n",
    "ttt = TicTacToe()\n",
    "# Loop for each episode:\n",
    "for ep in range(episodes):\n",
    "    # Initialize S\n",
    "    ttt.reset()\n",
    "    # Loop for each step of episode:\n",
    "    # until S is terminated\n",
    "    while True:\n",
    "        r = agent_1.move(ttt)\n",
    "        if r:\n",
    "            agent_2.lost()\n",
    "            win_history.append(r)\n",
    "            break\n",
    "\n",
    "        r = agent_2.move(ttt)\n",
    "        if r:\n",
    "            agent_1.lost()\n",
    "            win_history.append(1 - r)\n",
    "            break\n",
    "\n",
    "    if ep % 1000 == 0:\n",
    "        print('win or draw rate (last 1000 games) - {0:.02%}'.format(\n",
    "              np.mean(np.array(win_history[-1000:]) != 0.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 1 learns that it is optimal to start the game in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57454762, 0.56282788, 0.61569813],\n",
       "       [0.50418249, 0.81350581, 0.52416274],\n",
       "       [0.61962225, 0.51681234, 0.56206647]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "states, actions = ttt.available_action_states()\n",
    "values = [agent_1.value_dict.get(str(s.reshape((3, 3)).astype('int')), 0.5) for s in states]\n",
    "np.array(values).reshape((3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And agent 2 understands that he is doomed in such case and can not win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16927708, 0.06483458, 0.17500661],\n",
       "       [0.14831087, 0.        , 0.1324976 ],\n",
       "       [0.08615713, 0.10270979, 0.08706275]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "ttt.move(4)\n",
    "states, actions = ttt.available_action_states()\n",
    "values = [agent_2.value_dict.get(str(s.reshape((3, 3)).astype('int')), 0.5) for s in states]\n",
    "T = np.zeros((3, 3))\n",
    "for v, a in zip(values, actions):\n",
    "    T[a // 3, a % 3] = v\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "As you can see, self-play is a critical component if we want to end up with a smart agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep TD(0)\n",
    "\n",
    "Instead of using value dictionary we could try to use neural network. That might be usefull if the game is much more complex. In such case our value network expects board state as input and returns expected win probability. \n",
    "\n",
    "There is one big problem - while training NN we need to ensure that data is i.i.d. and it is not so if we update it during each step. Instead we will gather historical plays into a list and sample it randomly to train the network. This trick is called *experience replay* and is [vital component](https://deepmind.com/blog/article/replay-in-biological-and-artificial-neural-networks) in RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, exploration=0.9):\n",
    "        self.state = np.zeros((1, 9))\n",
    "        self.exploration = exploration\n",
    "        self.v = keras.models.Sequential()\n",
    "        self.v.add(keras.layers.Dense(9, activation='relu', input_shape=(9,)))\n",
    "        self.v.add(keras.layers.Dense(9, activation='relu'))\n",
    "        self.v.add(keras.layers.Dense(9, activation='relu'))\n",
    "        self.v.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "        self.v.compile(loss='mse', optimizer='adam')\n",
    "        self.hist = []\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.hist) > 90:\n",
    "            hist = [self.hist[idx] for idx in\n",
    "                    np.random.choice(range(len(self.hist)), 16 * 5)]\n",
    "            self.v.fit(np.vstack([h[0] for h in hist]),\n",
    "                   np.vstack([h[1] if type(h[1]) == int\n",
    "                              else self.v(h[1]) for h in hist]),\n",
    "                   verbose=0, batch_size=16)\n",
    "\n",
    "    def move(self, ttt):\n",
    "        if np.random.random() > self.exploration:\n",
    "            a = np.random.choice(ttt.available_actions())\n",
    "            r, new_state = ttt.move(a)\n",
    "            if r:    # game over\n",
    "                self.step()\n",
    "                return r\n",
    "        else:\n",
    "            # Choose the best action according to the stored values\n",
    "            states, actions = ttt.available_action_states()\n",
    "            a = actions[np.argmax(self.v(states))]\n",
    "            r, new_state = ttt.move(a)\n",
    "            self.hist.append([self.state, new_state])\n",
    "            if r:   # game over\n",
    "                self.state = new_state\n",
    "                for _ in range(5):\n",
    "                    self.hist.append([new_state, 1 if r == 1 else 0])\n",
    "                self.step()\n",
    "                return r\n",
    "        self.state = new_state\n",
    "        return 0.\n",
    "\n",
    "    def lost(self):\n",
    "        for _ in range(5):\n",
    "            self.hist.append([self.state, 0])\n",
    "        self.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are training NN it will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win or draw rate (last 1000 games) - 100.00%\n",
      "[[0.4916704  0.48525652 0.47802982]\n",
      " [0.48747227 0.49330837 0.48380294]\n",
      " [0.51800615 0.48726496 0.52977437]]\n",
      "[[0.50695962 0.53919619 0.57110035]\n",
      " [0.52761245 0.         0.56992632]\n",
      " [0.58140391 0.53762287 0.55702502]]\n",
      "win or draw rate (last 1000 games) - 70.80%\n",
      "[[0.19857982 0.5731377  0.22716764]\n",
      " [0.24610418 0.8355091  0.19589329]\n",
      " [0.3072345  0.10940447 0.5762914 ]]\n",
      "[[0.00848639 0.02569416 0.02345979]\n",
      " [0.01719093 0.         0.02280116]\n",
      " [0.00972328 0.03122777 0.0841741 ]]\n",
      "win or draw rate (last 1000 games) - 87.10%\n",
      "[[0.2558624  0.48461166 0.5048217 ]\n",
      " [0.4042833  0.924652   0.18331254]\n",
      " [0.33432338 0.0753983  0.44958222]]\n",
      "[[0.00572839 0.02248144 0.01194683]\n",
      " [0.01206854 0.         0.01220462]\n",
      " [0.00607294 0.02383551 0.05225745]]\n",
      "win or draw rate (last 1000 games) - 92.50%\n",
      "[[0.40035123 0.54496217 0.7670125 ]\n",
      " [0.48041937 0.9245795  0.17145503]\n",
      " [0.5033201  0.06989989 0.50753075]]\n",
      "[[0.00171757 0.00827879 0.00377765]\n",
      " [0.00448915 0.         0.00560331]\n",
      " [0.00186938 0.01140457 0.04909718]]\n",
      "win or draw rate (last 1000 games) - 94.30%\n",
      "[[0.55405456 0.49907514 0.79478425]\n",
      " [0.60511076 0.9764682  0.15467757]\n",
      " [0.61028516 0.06207812 0.5619402 ]]\n",
      "[[0.00067696 0.0036236  0.00148234]\n",
      " [0.00243032 0.         0.00312361]\n",
      " [0.00106969 0.00739855 0.02599847]]\n",
      "win or draw rate (last 1000 games) - 95.20%\n",
      "[[0.6284584  0.46854228 0.8208797 ]\n",
      " [0.6521456  0.97109735 0.11898199]\n",
      " [0.3457625  0.04561237 0.42221943]]\n",
      "[[0.00142226 0.00576219 0.00131583]\n",
      " [0.00481987 0.         0.00633064]\n",
      " [0.00144213 0.01076797 0.01400068]]\n",
      "win or draw rate (last 1000 games) - 94.10%\n",
      "[[0.8050296  0.489751   0.8788645 ]\n",
      " [0.72182167 0.9754179  0.13875425]\n",
      " [0.47207278 0.05375874 0.49411008]]\n",
      "[[0.00182512 0.0086399  0.00112212]\n",
      " [0.0077593  0.         0.00856066]\n",
      " [0.00193074 0.01376402 0.01478055]]\n",
      "win or draw rate (last 1000 games) - 87.40%\n",
      "[[0.6750169  0.42116672 0.81997144]\n",
      " [0.7628833  0.98143387 0.16021743]\n",
      " [0.5187866  0.04705718 0.4210554 ]]\n",
      "[[0.00334474 0.01778463 0.00178617]\n",
      " [0.01105863 0.         0.00800505]\n",
      " [0.00590411 0.02112225 0.01209691]]\n",
      "win or draw rate (last 1000 games) - 91.10%\n",
      "[[0.7669873  0.36637706 0.90033984]\n",
      " [0.6709896  0.990423   0.22330934]\n",
      " [0.41973612 0.04740518 0.36507422]]\n",
      "[[0.00517729 0.03302112 0.00217733]\n",
      " [0.01435807 0.         0.02991852]\n",
      " [0.01144838 0.03529805 0.01022997]]\n",
      "win or draw rate (last 1000 games) - 91.10%\n",
      "[[0.45975676 0.44303674 0.85523885]\n",
      " [0.47312692 0.9946798  0.192976  ]\n",
      " [0.3131255  0.08917099 0.42057443]]\n",
      "[[0.00182348 0.02626097 0.00140223]\n",
      " [0.01025584 0.         0.02912375]\n",
      " [0.00474551 0.01831719 0.00718066]]\n",
      "win or draw rate (last 1000 games) - 92.30%\n",
      "[[0.4421326  0.50690556 0.63432586]\n",
      " [0.45558614 0.9926535  0.26947683]\n",
      " [0.32076532 0.10975155 0.4054354 ]]\n",
      "[[0.00166827 0.01705    0.00143492]\n",
      " [0.01052824 0.         0.02090362]\n",
      " [0.00661147 0.0184623  0.00693658]]\n",
      "CPU times: user 22min 46s, sys: 28.5 s, total: 23min 14s\n",
      "Wall time: 23min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "episodes = 10_001\n",
    "agent_1 = Agent()\n",
    "agent_2 = Agent()\n",
    "\n",
    "win_history = []\n",
    "ttt = TicTacToe()\n",
    "# Loop for each episode:\n",
    "for ep in range(episodes):\n",
    "    # Initialize S\n",
    "    ttt.reset()\n",
    "    # Loop for each step of episode:\n",
    "    # until S is terminated\n",
    "    while True:\n",
    "        r = agent_1.move(ttt)\n",
    "        if r:\n",
    "            agent_2.lost()\n",
    "            win_history.append(r)\n",
    "            break\n",
    "\n",
    "        r = agent_2.move(ttt)\n",
    "        if r:\n",
    "            agent_1.lost()\n",
    "            win_history.append(1 - r)\n",
    "            break\n",
    "\n",
    "    if ep % 1000 == 0:\n",
    "        print('win or draw rate (last 1000 games) - {0:.02%}'.format(\n",
    "              np.mean(np.array(win_history[-1000:]) != 0.)))\n",
    "        ttt = TicTacToe()\n",
    "        states, actions = ttt.available_action_states()\n",
    "        print(agent_1.v(states).numpy().reshape((3, 3)))\n",
    "        ttt.move(4)\n",
    "        states, actions = ttt.available_action_states()\n",
    "        T = np.zeros((3, 3))\n",
    "        for w, a in zip(agent_2.v(states), actions):\n",
    "            T[a // 3, a % 3] = w\n",
    "        print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {}
   },
   "source": [
    "As you can see it was able to produce similar results as the dictionary approach. We could extend it further, for example AlphaZero like implementation can be found at [AlphaToe](https://github.com/DanielSlater/AlphaToe).\n",
    "\n",
    "For sure for tic-tac-toe using TD is an overkill. Keep in mind that what we did here was for illustrative purposes - the aim was to show that TD is a general principle that can be used in games. Similar principles are applied in TD-Backgammon, Alpha Zero, MuZero and other RL agents.\n",
    "\n",
    "## Where to go next?\n",
    "\n",
    "If you are interested to dig deeper:\n",
    "- Read what [deepmind](https://deepmind.com/blog/article/deep-reinforcement-learning) is doing.\n",
    "- Read what [openAI](https://openai.com/) is doing.\n",
    "- Watch [AlphaGO](https://www.imdb.com/title/tt6700846/) movie and read [paper](https://www.nature.com/articles/nature16961).\n",
    "- Read Barto & Sutton [book](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf).\n",
    "- Work through [Deep RL](https://github.com/trokas/Deep_RL), which contains more examples and intuitive lower level implementations. This [medium](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724) series is great.\n",
    "- Read book \"Deep Reinforcement Learning Hands-On\" by Maxim Laptan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
