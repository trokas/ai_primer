{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run following command to install deps:\n",
    "\n",
    "```pip install keras-rl2 gym```\n",
    "\n",
    "If you are interested dig deeper:\n",
    "- Read what [deepmind](https://deepmind.com/blog/article/deep-reinforcement-learning) is doing.\n",
    "- Read what [openAI](https://openai.com/) is doing.\n",
    "- Watch [AlphaGO](https://www.imdb.com/title/tt6700846/) movie and read [paper](https://www.nature.com/articles/nature16961).\n",
    "- Work through [Deep RL](https://github.com/trokas/Deep_RL), which contains more examples and intuitive lower level implementations. This [medium](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724) series is great.\n",
    "- Read book \"Deep Reinforcement Learning Hands-On\" by Maxim Laptan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from scipy.stats import beta\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandits\n",
    "\n",
    "First we need an experiment, for example let's set up 3 arm bandit so that first hand gives highest reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDITS = [0.7, 0.2, 0.3]  # arm 1 wins with prob of 50%\n",
    "\n",
    "def pull(i):\n",
    "    return 1 if np.random.rand() < BANDITS[i] else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement multi-armed bandit algorithm based on bayesian update\n",
    "\n",
    "$$P(\\theta | x)=\\frac{P(x | \\theta) P(\\theta)}{P(x)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pulls [17, 1, 2]\n",
      "Total wins  [14, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "pulls = [0 for _ in range(len(BANDITS))]  # How much pulls were executed\n",
    "wins =  [0 for _ in range(len(BANDITS))]  # Number of wins\n",
    "\n",
    "n = 20    # Number of pulls to do\n",
    "\n",
    "for _ in range(n):\n",
    "    priors = [beta(1 + w, 1 + p - w) for p, w in zip(pulls, wins)]\n",
    "    # Choose a 'best' bandit based on probabilities\n",
    "    chosen_arm = np.argmax([p.rvs(1) for p in priors])\n",
    "    # Pull and record to output\n",
    "    reward = pull(chosen_arm)\n",
    "    pulls[chosen_arm] += 1\n",
    "    wins[chosen_arm] += reward\n",
    "    \n",
    "print('Total pulls', pulls)\n",
    "print('Total wins ', wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(index=np.linspace(0, 1, 101))\n",
    "for i, p in enumerate(priors):\n",
    "    df[f'arm_{i}'] = p.pdf(df.index)\n",
    "\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** rerun with higher number of experiments. Do the distributions look better then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually only to exploit the system is not a good idea and *epsilon-greedy* can help to balance out explore/exploit.\n",
    "\n",
    "Idea of *epsilon-greedy* is simple:\n",
    "\n",
    "```python\n",
    "if random_number < epsilon:\n",
    "    # choose arm to pull randomly\n",
    "else:\n",
    "    # choose optimal arm based on pulls and wins\n",
    "```\n",
    "\n",
    "**TASK:** implement epsilon-greedy and run some experiments with different epsilon values. Try to reson when epsilon-greedy might be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-learning is a model-free RL algorithm to learn quality of actions telling an agent what action to take under what circumstances. Idea is simple - we will store values in state/action table and use it as a reference for making actions.\n",
    "\n",
    "Let's start by looking at markov decision process based game:\n",
    "\n",
    "<img src=\"img/markov_decision_process.png\" alt=\"Markov decision process\" style=\"width: 600px;\"/>\n",
    "\n",
    "This can be represented with transition weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [ # shape=[s, a, s']\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None]]\n",
    "rewards = [ # shape=[s, a, s']\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to run through iterative optimization process\n",
    "\n",
    "$$Q_{k+1} (s,a) \\leftarrow \\sum_{s'} T(s,a,s') [R(s,a,s') + \\gamma \\max_{a'} Q_k(s', a')] \\; \\text{for all} \\; (s'a).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions\n",
    "    \n",
    "gamma = 0.90 # the discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of using discounted rewards in Q-states is one of the fundamental ideas in RL. For sure we don't know initial probabilities and rewards, but as we will see we can learn them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole and DQN\n",
    "\n",
    "Get the environment and extract the number of actions.\n",
    "\n",
    "We will try to balance a stick - [CartPole](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "\n",
    "![](https://miro.medium.com/max/960/1*G_whtIrY9fGlw3It6HFfhA.gif)\n",
    "\n",
    "To meet provide this challenge we are going to utilize the [OpenAI gym](https://gym.openai.com/docs/), a collection of reinforcement learning environments.\n",
    "\n",
    "- Observations — The agent needs to know where pole currently is, and the angle at which it is balancing.\n",
    "- Delayed reward — Keeping the pole in the air as long as possible means moving in ways that will be advantageous for both the present and the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "nb_actions = env.action_space.n\n",
    "print('Number of actions', nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1, 4)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure and compile our agent. We will use Epsilon Greedy:\n",
    "- All actions initially are tried with non-zero probability\n",
    "- With probability $1-\\epsilon$ choose the greedy action\n",
    "- With probability $\\epsilon$ choose an action ar random\n",
    "\n",
    "and we will estimate target Q-Value using reward and the future discounted value estimate\n",
    "\n",
    "$$Q_{target}(s,a) = r + \\gamma \\cdot \\max_{a'} Q_\\theta (s', a').$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions,\n",
    "               memory=memory, nb_steps_warmup=10, \n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks like before training. Note, that pole does not have to fall fully for gym to note it as a failed play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "WARNING:tensorflow:From /Users/trokas/.local/share/virtualenvs/current-rcFo7dEP/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "Episode 1: reward: 9.000, steps: 9\n",
      "Episode 2: reward: 10.000, steps: 10\n",
      "Episode 3: reward: 8.000, steps: 8\n",
      "Episode 4: reward: 9.000, steps: 9\n",
      "Episode 5: reward: 9.000, steps: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14bf72d90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyglet.gl import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time to learn something! You can visualize the training by setting `visualize=True`, but this\n",
    "slows down training quite a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trokas/.local/share/virtualenvs/current-rcFo7dEP/lib/python3.7/site-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12/5000: episode: 1, duration: 0.519s, episode steps:  12, steps per second:  23, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.470474, mae: 0.588689, mean_q: -0.165078\n",
      "   22/5000: episode: 2, duration: 0.061s, episode steps:  10, steps per second: 164, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.430313, mae: 0.540784, mean_q: -0.092450\n",
      "   31/5000: episode: 3, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.335967, mae: 0.472474, mean_q: 0.008541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trokas/.local/share/virtualenvs/current-rcFo7dEP/lib/python3.7/site-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/Users/trokas/.local/share/virtualenvs/current-rcFo7dEP/lib/python3.7/site-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41/5000: episode: 4, duration: 0.081s, episode steps:  10, steps per second: 124, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.275959, mae: 0.449669, mean_q: 0.118187\n",
      "   51/5000: episode: 5, duration: 0.092s, episode steps:  10, steps per second: 108, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.222271, mae: 0.432547, mean_q: 0.234795\n",
      "   59/5000: episode: 6, duration: 0.051s, episode steps:   8, steps per second: 156, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.174522, mae: 0.418035, mean_q: 0.355626\n",
      "   69/5000: episode: 7, duration: 0.060s, episode steps:  10, steps per second: 167, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.141265, mae: 0.404698, mean_q: 0.483447\n",
      "   79/5000: episode: 8, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.109955, mae: 0.393378, mean_q: 0.658335\n",
      "   91/5000: episode: 9, duration: 0.079s, episode steps:  12, steps per second: 152, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.091450, mae: 0.396206, mean_q: 0.877767\n",
      "  100/5000: episode: 10, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.071080, mae: 0.367935, mean_q: 1.070451\n",
      "  110/5000: episode: 11, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.040656, mae: 0.277581, mean_q: 1.128758\n",
      "  122/5000: episode: 12, duration: 0.068s, episode steps:  12, steps per second: 175, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.039916, mae: 0.235055, mean_q: 1.220565\n",
      "  133/5000: episode: 13, duration: 0.064s, episode steps:  11, steps per second: 173, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.037617, mae: 0.185672, mean_q: 1.301775\n",
      "  142/5000: episode: 14, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.029914, mae: 0.136788, mean_q: 1.329070\n",
      "  153/5000: episode: 15, duration: 0.074s, episode steps:  11, steps per second: 149, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.020719, mae: 0.097963, mean_q: 1.405725\n",
      "  162/5000: episode: 16, duration: 0.052s, episode steps:   9, steps per second: 172, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.028612, mae: 0.103295, mean_q: 1.469801\n",
      "  172/5000: episode: 17, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.049274, mae: 0.166730, mean_q: 1.473215\n",
      "  180/5000: episode: 18, duration: 0.047s, episode steps:   8, steps per second: 171, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.050855, mae: 0.227547, mean_q: 1.582873\n",
      "  191/5000: episode: 19, duration: 0.071s, episode steps:  11, steps per second: 155, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.036931, mae: 0.293956, mean_q: 1.573139\n",
      "  201/5000: episode: 20, duration: 0.057s, episode steps:  10, steps per second: 176, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.037119, mae: 0.372745, mean_q: 1.674690\n",
      "  214/5000: episode: 21, duration: 0.086s, episode steps:  13, steps per second: 150, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.923 [0.000, 1.000],  loss: 0.031381, mae: 0.466767, mean_q: 1.717232\n",
      "  224/5000: episode: 22, duration: 0.062s, episode steps:  10, steps per second: 162, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.026674, mae: 0.542755, mean_q: 1.758591\n",
      "  235/5000: episode: 23, duration: 0.067s, episode steps:  11, steps per second: 164, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.027041, mae: 0.612402, mean_q: 1.860339\n",
      "  246/5000: episode: 24, duration: 0.063s, episode steps:  11, steps per second: 174, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.027587, mae: 0.699082, mean_q: 1.904512\n",
      "  256/5000: episode: 25, duration: 0.072s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.029212, mae: 0.805567, mean_q: 1.940700\n",
      "  266/5000: episode: 26, duration: 0.057s, episode steps:  10, steps per second: 174, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.025534, mae: 0.899284, mean_q: 2.023443\n",
      "  276/5000: episode: 27, duration: 0.056s, episode steps:  10, steps per second: 178, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.022218, mae: 0.966137, mean_q: 2.042806\n",
      "  286/5000: episode: 28, duration: 0.057s, episode steps:  10, steps per second: 177, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.021570, mae: 1.044661, mean_q: 2.155052\n",
      "  295/5000: episode: 29, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.020520, mae: 1.094059, mean_q: 2.199599\n",
      "  308/5000: episode: 30, duration: 0.077s, episode steps:  13, steps per second: 169, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.022571, mae: 1.133968, mean_q: 2.243752\n",
      "  317/5000: episode: 31, duration: 0.055s, episode steps:   9, steps per second: 164, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.020044, mae: 1.149221, mean_q: 2.304041\n",
      "  327/5000: episode: 32, duration: 0.066s, episode steps:  10, steps per second: 151, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.021239, mae: 1.169513, mean_q: 2.339834\n",
      "  336/5000: episode: 33, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.023094, mae: 1.188593, mean_q: 2.390890\n",
      "  346/5000: episode: 34, duration: 0.065s, episode steps:  10, steps per second: 154, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.022992, mae: 1.202299, mean_q: 2.471655\n",
      "  357/5000: episode: 35, duration: 0.068s, episode steps:  11, steps per second: 162, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.019110, mae: 1.238184, mean_q: 2.502657\n",
      "  365/5000: episode: 36, duration: 0.051s, episode steps:   8, steps per second: 157, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.019257, mae: 1.298393, mean_q: 2.608709\n",
      "  374/5000: episode: 37, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.022095, mae: 1.292124, mean_q: 2.564080\n",
      "  382/5000: episode: 38, duration: 0.049s, episode steps:   8, steps per second: 162, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.019151, mae: 1.287852, mean_q: 2.625877\n",
      "  390/5000: episode: 39, duration: 0.050s, episode steps:   8, steps per second: 162, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.017112, mae: 1.312830, mean_q: 2.695813\n",
      "  400/5000: episode: 40, duration: 0.056s, episode steps:  10, steps per second: 178, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.023879, mae: 1.328734, mean_q: 2.682201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  411/5000: episode: 41, duration: 0.082s, episode steps:  11, steps per second: 135, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.020045, mae: 1.381140, mean_q: 2.797448\n",
      "  420/5000: episode: 42, duration: 0.053s, episode steps:   9, steps per second: 170, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.019161, mae: 1.413261, mean_q: 2.860299\n",
      "  429/5000: episode: 43, duration: 0.051s, episode steps:   9, steps per second: 178, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.018604, mae: 1.456374, mean_q: 2.892892\n",
      "  439/5000: episode: 44, duration: 0.056s, episode steps:  10, steps per second: 178, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.020485, mae: 1.566899, mean_q: 3.080875\n",
      "  450/5000: episode: 45, duration: 0.069s, episode steps:  11, steps per second: 160, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.054166, mae: 1.594057, mean_q: 3.129970\n",
      "  469/5000: episode: 46, duration: 0.109s, episode steps:  19, steps per second: 175, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.737 [0.000, 1.000],  loss: 0.085353, mae: 1.639644, mean_q: 3.223077\n",
      "  479/5000: episode: 47, duration: 0.074s, episode steps:  10, steps per second: 135, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.074795, mae: 1.633099, mean_q: 3.251204\n",
      "  489/5000: episode: 48, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.064578, mae: 1.737262, mean_q: 3.414563\n",
      "  510/5000: episode: 49, duration: 0.132s, episode steps:  21, steps per second: 159, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.810 [0.000, 1.000],  loss: 0.098541, mae: 1.766980, mean_q: 3.486397\n",
      "  519/5000: episode: 50, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.023946, mae: 1.777427, mean_q: 3.657187\n",
      "  528/5000: episode: 51, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.225973, mae: 1.852565, mean_q: 3.621992\n",
      "  538/5000: episode: 52, duration: 0.058s, episode steps:  10, steps per second: 173, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.073033, mae: 1.788411, mean_q: 3.667327\n",
      "  547/5000: episode: 53, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.143109, mae: 1.906599, mean_q: 3.800969\n",
      "  557/5000: episode: 54, duration: 0.059s, episode steps:  10, steps per second: 171, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.116130, mae: 1.945354, mean_q: 3.823783\n",
      "  567/5000: episode: 55, duration: 0.066s, episode steps:  10, steps per second: 152, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.060006, mae: 1.932278, mean_q: 3.874469\n",
      "  581/5000: episode: 56, duration: 0.078s, episode steps:  14, steps per second: 180, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.055350, mae: 2.012805, mean_q: 3.915246\n",
      "  603/5000: episode: 57, duration: 0.124s, episode steps:  22, steps per second: 177, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.773 [0.000, 1.000],  loss: 0.169755, mae: 2.083003, mean_q: 4.036840\n",
      "  615/5000: episode: 58, duration: 0.078s, episode steps:  12, steps per second: 154, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.114085, mae: 2.029571, mean_q: 4.084495\n",
      "  624/5000: episode: 59, duration: 0.053s, episode steps:   9, steps per second: 170, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.036830, mae: 2.182385, mean_q: 4.291920\n",
      "  633/5000: episode: 60, duration: 0.053s, episode steps:   9, steps per second: 171, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.079529, mae: 2.263263, mean_q: 4.401288\n",
      "  645/5000: episode: 61, duration: 0.076s, episode steps:  12, steps per second: 157, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.119164, mae: 2.326660, mean_q: 4.528762\n",
      "  659/5000: episode: 62, duration: 0.077s, episode steps:  14, steps per second: 183, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.157494, mae: 2.281876, mean_q: 4.494839\n",
      "  672/5000: episode: 63, duration: 0.072s, episode steps:  13, steps per second: 180, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.169453, mae: 2.292716, mean_q: 4.462337\n",
      "  681/5000: episode: 64, duration: 0.053s, episode steps:   9, steps per second: 170, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.151908, mae: 2.424157, mean_q: 4.723896\n",
      "  695/5000: episode: 65, duration: 0.089s, episode steps:  14, steps per second: 158, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.304582, mae: 2.426633, mean_q: 4.590652\n",
      "  705/5000: episode: 66, duration: 0.057s, episode steps:  10, steps per second: 174, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.436756, mae: 2.424292, mean_q: 4.715697\n",
      "  716/5000: episode: 67, duration: 0.066s, episode steps:  11, steps per second: 167, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.098687, mae: 2.408761, mean_q: 4.801852\n",
      "  734/5000: episode: 68, duration: 0.104s, episode steps:  18, steps per second: 173, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.155669, mae: 2.528998, mean_q: 4.861622\n",
      "  744/5000: episode: 69, duration: 0.057s, episode steps:  10, steps per second: 176, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.325872, mae: 2.618933, mean_q: 4.982594\n",
      "  753/5000: episode: 70, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.138682, mae: 2.574592, mean_q: 5.071961\n",
      "  824/5000: episode: 71, duration: 0.405s, episode steps:  71, steps per second: 175, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 0.209768, mae: 2.749775, mean_q: 5.309204\n",
      "  843/5000: episode: 72, duration: 0.135s, episode steps:  19, steps per second: 140, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.499627, mae: 2.907677, mean_q: 5.520334\n",
      "  866/5000: episode: 73, duration: 0.138s, episode steps:  23, steps per second: 167, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.352647, mae: 3.020049, mean_q: 5.775416\n",
      "  904/5000: episode: 74, duration: 0.212s, episode steps:  38, steps per second: 180, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.480730, mae: 3.109391, mean_q: 5.915252\n",
      "  920/5000: episode: 75, duration: 0.100s, episode steps:  16, steps per second: 160, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.531046, mae: 3.173185, mean_q: 5.956018\n",
      "  942/5000: episode: 76, duration: 0.148s, episode steps:  22, steps per second: 149, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.377572, mae: 3.220942, mean_q: 6.163136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  978/5000: episode: 77, duration: 0.209s, episode steps:  36, steps per second: 172, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.431317, mae: 3.425709, mean_q: 6.535269\n",
      " 1027/5000: episode: 78, duration: 0.265s, episode steps:  49, steps per second: 185, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.612 [0.000, 1.000],  loss: 0.423559, mae: 3.513860, mean_q: 6.769080\n",
      " 1059/5000: episode: 79, duration: 0.181s, episode steps:  32, steps per second: 177, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.658946, mae: 3.776556, mean_q: 7.246701\n",
      " 1070/5000: episode: 80, duration: 0.083s, episode steps:  11, steps per second: 132, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.327387, mae: 3.787353, mean_q: 7.344775\n",
      " 1088/5000: episode: 81, duration: 0.115s, episode steps:  18, steps per second: 156, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.316476, mae: 3.937152, mean_q: 7.609412\n",
      " 1104/5000: episode: 82, duration: 0.106s, episode steps:  16, steps per second: 151, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.420196, mae: 3.957492, mean_q: 7.688070\n",
      " 1132/5000: episode: 83, duration: 0.183s, episode steps:  28, steps per second: 153, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.426044, mae: 4.046975, mean_q: 7.878060\n",
      " 1150/5000: episode: 84, duration: 0.113s, episode steps:  18, steps per second: 159, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.827794, mae: 4.221570, mean_q: 8.054273\n",
      " 1200/5000: episode: 85, duration: 0.284s, episode steps:  50, steps per second: 176, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 0.714621, mae: 4.314801, mean_q: 8.287112\n",
      " 1247/5000: episode: 86, duration: 0.258s, episode steps:  47, steps per second: 182, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 0.773649, mae: 4.538174, mean_q: 8.704104\n",
      " 1261/5000: episode: 87, duration: 0.076s, episode steps:  14, steps per second: 184, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.502036, mae: 4.533714, mean_q: 8.829727\n",
      " 1308/5000: episode: 88, duration: 0.261s, episode steps:  47, steps per second: 180, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 0.570172, mae: 4.647412, mean_q: 9.040591\n",
      " 1370/5000: episode: 89, duration: 0.379s, episode steps:  62, steps per second: 164, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.989270, mae: 4.946057, mean_q: 9.515224\n",
      " 1399/5000: episode: 90, duration: 0.159s, episode steps:  29, steps per second: 183, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.176853, mae: 5.198234, mean_q: 9.959958\n",
      " 1448/5000: episode: 91, duration: 0.309s, episode steps:  49, steps per second: 158, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 0.952976, mae: 5.287143, mean_q: 10.154800\n",
      " 1498/5000: episode: 92, duration: 0.405s, episode steps:  50, steps per second: 123, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.870449, mae: 5.336462, mean_q: 10.413436\n",
      " 1664/5000: episode: 93, duration: 1.245s, episode steps: 166, steps per second: 133, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.958424, mae: 5.705412, mean_q: 11.109818\n",
      " 1826/5000: episode: 94, duration: 0.922s, episode steps: 162, steps per second: 176, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.992422, mae: 6.281146, mean_q: 12.353326\n",
      " 1924/5000: episode: 95, duration: 0.558s, episode steps:  98, steps per second: 176, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.852377, mae: 6.789612, mean_q: 13.442488\n",
      " 2046/5000: episode: 96, duration: 0.770s, episode steps: 122, steps per second: 159, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.885076, mae: 7.222069, mean_q: 14.393581\n",
      " 2153/5000: episode: 97, duration: 0.687s, episode steps: 107, steps per second: 156, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.367149, mae: 7.668120, mean_q: 15.262555\n",
      " 2279/5000: episode: 98, duration: 0.713s, episode steps: 126, steps per second: 177, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.260647, mae: 8.097031, mean_q: 16.164738\n",
      " 2396/5000: episode: 99, duration: 0.686s, episode steps: 117, steps per second: 171, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.178987, mae: 8.592623, mean_q: 17.307045\n",
      " 2506/5000: episode: 100, duration: 0.686s, episode steps: 110, steps per second: 160, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.449976, mae: 9.069571, mean_q: 18.257244\n",
      " 2638/5000: episode: 101, duration: 0.706s, episode steps: 132, steps per second: 187, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.439077, mae: 9.681490, mean_q: 19.509804\n",
      " 2838/5000: episode: 102, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.288759, mae: 10.281319, mean_q: 20.763508\n",
      " 2966/5000: episode: 103, duration: 0.746s, episode steps: 128, steps per second: 172, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 1.021335, mae: 10.965878, mean_q: 22.229786\n",
      " 3085/5000: episode: 104, duration: 0.614s, episode steps: 119, steps per second: 194, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.724188, mae: 11.502838, mean_q: 23.207365\n",
      " 3240/5000: episode: 105, duration: 0.935s, episode steps: 155, steps per second: 166, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.152186, mae: 11.955059, mean_q: 24.223032\n",
      " 3351/5000: episode: 106, duration: 0.765s, episode steps: 111, steps per second: 145, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.526762, mae: 12.509210, mean_q: 25.333750\n",
      " 3503/5000: episode: 107, duration: 0.819s, episode steps: 152, steps per second: 186, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.491376, mae: 12.957224, mean_q: 26.248934\n",
      " 3648/5000: episode: 108, duration: 0.802s, episode steps: 145, steps per second: 181, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.509259, mae: 13.522982, mean_q: 27.380350\n",
      " 3796/5000: episode: 109, duration: 0.749s, episode steps: 148, steps per second: 198, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.833841, mae: 14.026245, mean_q: 28.358576\n",
      " 3933/5000: episode: 110, duration: 0.747s, episode steps: 137, steps per second: 183, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 1.839880, mae: 14.553422, mean_q: 29.404638\n",
      " 4089/5000: episode: 111, duration: 1.021s, episode steps: 156, steps per second: 153, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.253473, mae: 14.786485, mean_q: 29.928009\n",
      " 4221/5000: episode: 112, duration: 0.693s, episode steps: 132, steps per second: 191, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 1.984523, mae: 15.235884, mean_q: 30.869091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4354/5000: episode: 113, duration: 0.721s, episode steps: 133, steps per second: 184, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 2.111134, mae: 15.772705, mean_q: 32.011673\n",
      " 4498/5000: episode: 114, duration: 0.878s, episode steps: 144, steps per second: 164, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.185027, mae: 16.169411, mean_q: 32.845894\n",
      " 4643/5000: episode: 115, duration: 0.847s, episode steps: 145, steps per second: 171, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.088379, mae: 16.734930, mean_q: 33.954765\n",
      " 4775/5000: episode: 116, duration: 0.753s, episode steps: 132, steps per second: 175, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.871243, mae: 17.063446, mean_q: 34.656677\n",
      " 4941/5000: episode: 117, duration: 0.941s, episode steps: 166, steps per second: 176, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.217524, mae: 17.468794, mean_q: 35.423603\n",
      "done, took 30.338 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15108fb10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our reinforcement learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 157.000, steps: 157\n",
      "Episode 2: reward: 140.000, steps: 140\n",
      "Episode 3: reward: 134.000, steps: 134\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 143.000, steps: 143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15269e050>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nearly a perfect play, since CartPole exits if 200 steps are reached. You can experiment with version which limit is 500 by changing env to `CartPole-v1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuZero\n",
    "\n",
    "Recent and impressive advancement in AI is MuZero. We will just run the implementation listed in - https://github.com/werner-duvaud/muzero-general. General idea involves first constructing the embedding and only then training the agent. According to George Hotz this algorithm is the one of the most important event in the history AI that people will cite for ages.\n",
    "\n",
    "<img width=\"60%\" src=\"https://miro.medium.com/max/2424/1*NowOwxV5SQ9aLKbjdz41lQ.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-11 14:57:51,604\tERROR worker.py:660 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "Run tensorboard --logdir ./results and go to http://localhost:6006/ to see in real time the training performance.\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=33338)\u001b[0m You are not training on GPU.\n",
      "\u001b[2m\u001b[36m(pid=33338)\u001b[0m \n",
      "Last test reward: 386.00. Training step: 10000/10000. Played games: 68. Loss: 5.89\n",
      "Shutting down workers...\n",
      "\n",
      "\n",
      "Persisting replay buffer games to disk...\n"
     ]
    }
   ],
   "source": [
    "# I just downloaded it from git and installed deps\n",
    "import os\n",
    "os.chdir('/Users/trokas/muzero-general/')\n",
    "\n",
    "from muzero import MuZero\n",
    "\n",
    "muzero = MuZero(\"cartpole\")  # it uses v1 by default\n",
    "muzero.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only with 68 it achieved decent result!\n",
    "\n",
    "If you want interesting challenge you could try to add additional experiment and insted of using cartpole position as input pass image of the cartpole instead. That's where the power of MuZero can be seen, it should be able to construct internal representation suitable for learning on it's own!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
